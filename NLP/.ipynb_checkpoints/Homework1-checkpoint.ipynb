{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1: Twenty Small Python/NLTK Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Out: Tuesday, February 11\n",
    "## Due Date: Thursday, February 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This introductory programming assignment is composed Python exercises, ranging from introductory to more advanced (OO, inheritance, classes). Python is not a difficult language to learn (as long as you already know a procedural language like Java or C++). However, its syntax is different and there are a handful of very unique features, some of which come in very handy for text processing. You'll discover that you can write powerful Python programs in less lines than would be required in Java.\n",
    "\n",
    "We've already discussed that Python is not a prerequisite for this class, and while we will introduce and practice some aspects of Python in-class, you are expected to google, learn, and practice any other areas of Python that you'll need on your own. Don't be afraid to figure out how to do something!\n",
    "\n",
    "This homework should prepare you for more complicated programming in the weeks ahead. More formally, the objectives of this homework are:\n",
    "* Learning the basics of Python through hands-on programming\n",
    "* Getting comfortable with a Jupyter notebook environment\n",
    "* Testing your code by using provided test cases\n",
    "\n",
    "This is an individual programming assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Resources\n",
    "\n",
    "See the Python resources that I provided on the main coursepage, in week 1 under \"Resources\".\n",
    "I also use google a lot to answer my Python questions -- however, be sure that the \"answers\" that you are looking at are for Python 3.x code. (Version 2.x is still very popular, and thus, often is returned as a higher search result.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "\n",
    "I will primarily be running your notebook against unit tests.\n",
    "\n",
    "_More passing tests = higher grade!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission instructions\n",
    "\n",
    "Submit your `.ipynb` notebook file to the D2L Assignments folder for this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define a function <code>is_palindrome()</code> that recognizes palindromes (i.e. words that read the same backward or foreward). For example, <code>is_palindrome(\"radar\")</code> should return <samp>True</samp>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_palindrome(myStr):\n",
    "    myStr = myStr.replace(\" \", \"\").lower()\n",
    "    return myStr == myStr[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_palindrome(\"tattarrattat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function named <code>odd_list</code> that takes a list of numbers and returns a new list that only contains those numbers that are odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd_list(numLst):\n",
    "    return [n for n in numLst if n % 2 > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7]"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_list([1,2,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function called <code>is_sorted</code> that takes a list as a parameter and returns <samp>True</samp> if the list is sorted in ascending order and <samp>False</samp> otherwise. You can assume (as a precondition) that the elements of the list can be compared with the relational operators `<`, `>`, etc. For example, `is_sorted([1,2,2])` should return `True` and `is_sorted(['b','a'])` should return `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sorted(myLst):\n",
    "    return myLst == sorted(myLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_sorted(['a','b','a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write a function `starts_with_vowel` that takes a character (a string of length 1) and returns `True` if it is a vowel, `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starts_with_vowel(myStr):\n",
    "    return myStr.lower().startswith(('a','e','i','o','u'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts_with_vowel(\"Ollie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Two words are anagrams if you can rearrange the letters from one to spell the other. Write a function called <code>is_anagram</code> that takes two strings and returns <samp>True</samp> if they are anagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_anagram(str1, str2):\n",
    "    str1 = str1.replace(\" \", \"\").lower()\n",
    "    str2 = str2.replace(\" \", \"\").lower()\n",
    "    return sorted(str1) == sorted(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_anagram(\"school master\", \"the classroom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. A pangram is a sentence that contains all the letters of the English alphabet at least once, for example: <samp>The quick brown fox jumps over the lazy dog</samp>. Write a function <code>is_pangram</code> to check a sentence to see if it is a pangram or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pangram(myStr):\n",
    "    myStr = myStr.lower()\n",
    "    alphabetSet = set([chr(x) for x in range(ord('a'), ord('z') + 1)])\n",
    "    return set(myStr) >= alphabetSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_pangram(\"The quick brown fox jumps over the lazy dog abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write a function `char_freq()` that takes a string and builds a frequency listing of the characters contained in it. Represent the frequency listing as a Python dictionary that the function returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_freq(myStr):\n",
    "    freq_dict = {}\n",
    "    myStr = myStr.lower()\n",
    "    strSet = set(myStr)\n",
    "    [freq_dict.update({c: 0}) for c in strSet]\n",
    "    [freq_dict.update({c: freq_dict[c] + 1}) for c in myStr]\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 2, 'e': 3, 'g': 1, 'i': 1, 'n': 1, 's': 3, 't': 6}"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_freq(\"testing test test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. In cryptography, a Caesar cipher is a very simple encryption techniques in which each letter in the plain text is replaced by a letter some fixed number of positions down the alphabet. For example, with a shift of 3, `A` would be replaced by `D`, `B` would become `E`, and so on. The method is named after Julius Caesar, who used it to communicate with his generals. <u>ROT-13</u> (\"rotate by 13 places\") is a widely used example of a Caesar cipher where the shift is 13. In Python, the key for ROT-13 may be represented by means of the following dictionary:\n",
    "\n",
    "```python\n",
    "key = {'a':'n', 'b':'o', 'c':'p', 'd':'q', 'e':'r', 'f':'s', 'g':'t', 'h':'u', \n",
    "       'i':'v', 'j':'w', 'k':'x', 'l':'y', 'm':'z', 'n':'a', 'o':'b', 'p':'c', \n",
    "       'q':'d', 'r':'e', 's':'f', 't':'g', 'u':'h', 'v':'i', 'w':'j', 'x':'k',\n",
    "       'y':'l', 'z':'m', 'A':'N', 'B':'O', 'C':'P', 'D':'Q', 'E':'R', 'F':'S', \n",
    "       'G':'T', 'H':'U', 'I':'V', 'J':'W', 'K':'X', 'L':'Y', 'M':'Z', 'N':'A', \n",
    "       'O':'B', 'P':'C', 'Q':'D', 'R':'E', 'S':'F', 'T':'G', 'U':'H', 'V':'I', \n",
    "       'W':'J', 'X':'K', 'Y':'L', 'Z':'M'}\n",
    "```\n",
    "\n",
    "Write two functions: `encode` and `decode`, which take a string and return that string encoded/decoded using ROT-13. For example, the string `To be or not to be, That is the question` would be encoded to `Gb or be abg gb or, Gung vf gur dhrfgvba` using <u>ROT-13</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(myStr):\n",
    "    encodedStr = \"\"\n",
    "    for c in myStr:\n",
    "        index = ord(c)\n",
    "        if (index >= ord('a') and index <= ord('z')) or (index >= ord('A') and index <= ord('Z')):\n",
    "            index = index + 13\n",
    "            if index > ord('z') or (ord(c) < ord('a') and index > ord('Z')): \n",
    "                index = index - 26                \n",
    "        encodedStr += chr(index)\n",
    "    return encodedStr\n",
    "\n",
    "def decode(myStr):\n",
    "    decodedStr = \"\"\n",
    "    for c in myStr:\n",
    "        index = ord(c)\n",
    "        if (index >= ord('a') and index <= ord('z')) or (index >= ord('A') and index <= ord('Z')):\n",
    "            index = index - 13\n",
    "            if index < ord('A') or (ord(c) > ord('Z') and index < ord('a')): \n",
    "                index = index + 26                \n",
    "        decodedStr += chr(index)\n",
    "    return decodedStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To be or not to be, That is the question'"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(\"Gb or be abg gb or, Gung vf gur dhrfgvba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Write a function `word_length` that maps a list of words into a list of integers representing the lengths of the correponding words. Use list comprehension to iterate over the list, rather than a \"usual loop\". You must use list comprehension for credit. (Hint: the body of your function should be one line of code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_length(myList):\n",
    "    return [len(w) for w in myList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 2]"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_length([\"test\",\"hello\",\"ok\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Write a function named `filter_long_words` that takes a list of words and an integer `n` and returns the list of words, CAPITALIZED, that are longer than `n`. Use list comprehension again, like the last problem, instead of a \"usual loop\". You must use list comprehension for credit. (Hint: the body of your function should be one line of code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_long_words(myList, n):\n",
    "    return [w.upper() for w in myList if len(w) > n]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LONGWORD1', 'LONGWORD2']"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_long_words([\"hello\",\"longword1\", \"longword2\", \"short\", \"ok\"], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Write a function `password_check` that checks the validity of password input by users. The function should accept the password as a string argument, and return True for valid, or False for invalid. The following criteria should be used to check the password:\n",
    "  * it contains at least 1 uppercase letter\n",
    "  * it contains at least 1 number\n",
    "  * it contains at least 1 lowercase letter\n",
    "  * it contains at least 1 of the following symbols: ^@#$%\n",
    "  * it has at least 6 characters\n",
    "  * it has at most 12 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def password_check(pwd):\n",
    "    return bool(re.search(\"^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)(?=.*[^@#$%])[A-Za-z\\d^@#$%]{6,12}$\", pwd))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "password_check(\"Password1$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Write a function `b_alphabetical` that takes one argument (a list of words) and returns as a list all the words that begin with the letter `b`, in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_alphabetical(myList):\n",
    "    return sorted([c for c in myList if c.startswith('b')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baseball', 'bat', 'boring', 'boxing']"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_alphabetical(['bat','baseball','is','not', 'boxing', 'boring'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Write a function `text_select` that finds all words in a specified NLTK text (an `Nltk.Text` object: one of text1, text2, ... text9) that meet the following conditions:\n",
    "  * ending in ize\n",
    "  * containing the letter z\n",
    "  * containing the sequence of letters pt\n",
    "  * are all lowercase, except the first character is a capital letter\n",
    "  \n",
    "Return these words as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def text_select(nltkText):\n",
    "    return [w for w in nltkText if \"pt\" in w and w[0].isupper() and w[1:].islower() and w.endswith(\"ize\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tesptize', 'Axptxize']"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_select(nltk.Text([\"test\", \"Tesptize\", \"TeSptize\", \"tesptize\", \"Axptxize\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Write a function `vocab_size` that takes as an argument an NLTK text, and returns the vocabulary size of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/basement/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"book\")\n",
    "def vocab_size(nltkText):\n",
    "    return len([w for w in set(nltkText) if (bool(re.search(\"^[a-zA-Z]*$\", w.lower())))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9792"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import text4\n",
    "vocab_size(nltk.book.text4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Read in the texts of the State of the Union addresses, using the `state_union` corpus reader. Count occurrences of men, women, and people in each document. Return this information as a dictionary, which the following representation: for each dictionary entry, set the key to be one of the State of the Union addresses, and let the value be a 3-item tuple, which is the number of men, women, and people in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_union():\n",
    "    stateDict = {}\n",
    "    for fileid in nltk.corpus.state_union.fileids():\n",
    "        stateDict.update({fileid: (0,0,0)})\n",
    "        for w in nltk.corpus.state_union.words(fileid):\n",
    "            if (w.lower().startswith(\"men\") or w.lower().startswith(\"women\") or w.lower().startswith(\"people\")) == False:\n",
    "                continue                \n",
    "            tup = stateDict[fileid]\n",
    "            if (w.lower().startswith(\"men\")):\n",
    "                tmp = list(tup)\n",
    "                tmp[0] += 1\n",
    "                tup = tuple(tmp)\n",
    "            if (w.lower().startswith(\"women\")):\n",
    "                tmp = list(tup)\n",
    "                tmp[1] += 1\n",
    "                tup = tuple(tmp)\n",
    "            if (w.lower().startswith(\"people\")):\n",
    "                tmp = list(tup)\n",
    "                tmp[2] += 1\n",
    "                tup = tuple(tmp)\n",
    "            stateDict.update({fileid: tup})   \n",
    "    return stateDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1945-Truman.txt': (2, 2, 10),\n",
       " '1946-Truman.txt': (16, 7, 54),\n",
       " '1947-Truman.txt': (8, 2, 17),\n",
       " '1948-Truman.txt': (5, 1, 24),\n",
       " '1949-Truman.txt': (2, 1, 18),\n",
       " '1950-Truman.txt': (6, 2, 23),\n",
       " '1951-Truman.txt': (9, 2, 15),\n",
       " '1953-Eisenhower.txt': (5, 0, 20),\n",
       " '1954-Eisenhower.txt': (4, 0, 16),\n",
       " '1955-Eisenhower.txt': (9, 0, 31),\n",
       " '1956-Eisenhower.txt': (5, 2, 31),\n",
       " '1957-Eisenhower.txt': (7, 2, 17),\n",
       " '1958-Eisenhower.txt': (3, 1, 21),\n",
       " '1959-Eisenhower.txt': (5, 1, 17),\n",
       " '1960-Eisenhower.txt': (2, 0, 18),\n",
       " '1961-Kennedy.txt': (6, 0, 13),\n",
       " '1962-Kennedy.txt': (7, 2, 10),\n",
       " '1963-Johnson.txt': (1, 0, 3),\n",
       " '1963-Kennedy.txt': (13, 5, 14),\n",
       " '1964-Johnson.txt': (3, 1, 3),\n",
       " '1965-Johnson-1.txt': (10, 0, 16),\n",
       " '1965-Johnson-2.txt': (12, 3, 14),\n",
       " '1966-Johnson.txt': (14, 1, 36),\n",
       " '1967-Johnson.txt': (13, 1, 30),\n",
       " '1968-Johnson.txt': (4, 0, 18),\n",
       " '1969-Johnson.txt': (5, 2, 6),\n",
       " '1970-Nixon.txt': (3, 0, 24),\n",
       " '1971-Nixon.txt': (1, 0, 34),\n",
       " '1972-Nixon.txt': (1, 0, 9),\n",
       " '1973-Nixon.txt': (1, 0, 10),\n",
       " '1974-Nixon.txt': (0, 0, 22),\n",
       " '1975-Ford.txt': (0, 0, 14),\n",
       " '1976-Ford.txt': (3, 1, 18),\n",
       " '1977-Ford.txt': (2, 1, 20),\n",
       " '1978-Carter.txt': (0, 1, 26),\n",
       " '1979-Carter.txt': (0, 1, 16),\n",
       " '1980-Carter.txt': (1, 2, 13),\n",
       " '1981-Reagan.txt': (3, 1, 11),\n",
       " '1982-Reagan.txt': (1, 2, 18),\n",
       " '1983-Reagan.txt': (5, 7, 20),\n",
       " '1984-Reagan.txt': (3, 5, 27),\n",
       " '1985-Reagan.txt': (1, 1, 12),\n",
       " '1986-Reagan.txt': (3, 2, 14),\n",
       " '1987-Reagan.txt': (1, 0, 24),\n",
       " '1988-Reagan.txt': (4, 0, 17),\n",
       " '1989-Bush.txt': (4, 3, 13),\n",
       " '1990-Bush.txt': (3, 2, 10),\n",
       " '1991-Bush-1.txt': (2, 2, 15),\n",
       " '1991-Bush-2.txt': (8, 7, 13),\n",
       " '1992-Bush.txt': (4, 4, 27),\n",
       " '1993-Clinton.txt': (1, 2, 45),\n",
       " '1994-Clinton.txt': (1, 1, 67),\n",
       " '1995-Clinton.txt': (3, 3, 73),\n",
       " '1996-Clinton.txt': (2, 3, 43),\n",
       " '1997-Clinton.txt': (1, 2, 31),\n",
       " '1998-Clinton.txt': (2, 2, 22),\n",
       " '1999-Clinton.txt': (4, 3, 22),\n",
       " '2000-Clinton.txt': (11, 7, 41),\n",
       " '2001-GWBush-1.txt': (5, 3, 15),\n",
       " '2001-GWBush-2.txt': (1, 3, 12),\n",
       " '2002-GWBush.txt': (4, 6, 14),\n",
       " '2003-GWBush.txt': (10, 4, 34),\n",
       " '2004-GWBush.txt': (10, 8, 21),\n",
       " '2005-GWBush.txt': (8, 11, 18),\n",
       " '2006-GWBush.txt': (7, 7, 22)}"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_union()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Write a function `fifty` that takes an argument, an NLTK text, and returns as a list the 50 most frequently occurring words of the text that are not stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "from nltk.corpus import stopwords \n",
    "def fifty(nltkText):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [w for w in nltkText if not w in stop_words] \n",
    "    fdist = FreqDist(filtered_text)\n",
    "    return [list(w)[0] for w in fdist.most_common(50)] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'s\", '*T*-1', '*U*', '$']"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fifty(nltk.book.text7)[5:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Write a function `novel_10` that takes a NLTK text argument and returns in a list any word that appeared in the last 10% of the text that had not been encountered earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def novel_10(nltkText):\n",
    "    last10Index = math.floor(len(nltkText) * 0.9)\n",
    "    first90Set = set(nltkText[:last10Index-1])\n",
    "    return [w for w in nltkText[last10Index:] if (w in first90Set) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rat']"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novel_10(novel_10(nltk.Text(['the','cat','in','the','hat','ate','the','big','hairy','and','gray','rat','hat'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Write a function `sent_info` that takes a sentence expressed as a single string, splits it and counts up the words. Have the function print out each word and the word's frequency, one per line, in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_info(mySent):\n",
    "    fdist = FreqDist(nltk.Text(mySent.split()))\n",
    "    fdcom = sorted(fdist.most_common())\n",
    "    for w in fdcom:\n",
    "        w = list(w)\n",
    "        print(w[0] + \" \" + str(w[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1\n",
      "are 1\n",
      "in 1\n",
      "is 1\n",
      "test 3\n",
      "test, 1\n",
      "testing 1\n",
      "things 1\n",
      "this 2\n",
      "we 1\n"
     ]
    }
   ],
   "source": [
    "sent_info(\"test test this is a test, we are testing things in this test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Write a function `unknown` that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using `re.findall()`) and remove any items from this set that occur in the Words Corpus (`nltk.corpus.words`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from nltk.corpus import words\n",
    "def unknown(url):\n",
    "    req = requests.get(url)\n",
    "    req.encoding = \"utf-8\"\n",
    "    contents = req.text\n",
    "    wordsList = set(words.words())\n",
    "    tmpList = re.findall(r'\\b[a-z]+', contents)\n",
    "    return sorted(set([w for w in tmpList if (w in wordsList) == False]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aax',\n",
       " 'abbr',\n",
       " 'ac',\n",
       " 'adb',\n",
       " 'adbanner',\n",
       " 'adbody',\n",
       " 'adchoices',\n",
       " 'adcontainer',\n",
       " 'adfuel',\n",
       " 'adkey',\n",
       " 'adnxs',\n",
       " 'ads',\n",
       " 'adsection',\n",
       " 'adsystem',\n",
       " 'adtop',\n",
       " 'adv',\n",
       " 'adzones',\n",
       " 'affects',\n",
       " 'affirming',\n",
       " 'africa',\n",
       " 'ajax',\n",
       " 'akamaihd',\n",
       " 'alertshub',\n",
       " 'allowed',\n",
       " 'amazon',\n",
       " 'amazonaws',\n",
       " 'amd',\n",
       " 'america',\n",
       " 'americas',\n",
       " 'amp',\n",
       " 'amphtml',\n",
       " 'amznkey',\n",
       " 'androied',\n",
       " 'animations',\n",
       " 'antares',\n",
       " 'antialiased',\n",
       " 'anytime',\n",
       " 'api',\n",
       " 'app',\n",
       " 'appembed',\n",
       " 'appia',\n",
       " 'apps',\n",
       " 'apstag',\n",
       " 'arabic',\n",
       " 'archives',\n",
       " 'arguments',\n",
       " 'arts',\n",
       " 'asia',\n",
       " 'aspx',\n",
       " 'async',\n",
       " 'attempts',\n",
       " 'attr',\n",
       " 'augw',\n",
       " 'australia',\n",
       " 'auth',\n",
       " 'authtoken',\n",
       " 'autoplay',\n",
       " 'autoplaysinline',\n",
       " 'autos',\n",
       " 'autostart',\n",
       " 'avatar',\n",
       " 'azjg',\n",
       " 'backface',\n",
       " 'barstyles',\n",
       " 'baseline',\n",
       " 'bb',\n",
       " 'bd',\n",
       " 'bea',\n",
       " 'bemtay',\n",
       " 'bestoftv',\n",
       " 'bezier',\n",
       " 'bf',\n",
       " 'bfbfbf',\n",
       " 'bfff',\n",
       " 'bfontface',\n",
       " 'bg',\n",
       " 'bicubic',\n",
       " 'bk',\n",
       " 'bleacherreport',\n",
       " 'blockquote',\n",
       " 'blog',\n",
       " 'blogs',\n",
       " 'boldit',\n",
       " 'boomtrain',\n",
       " 'bootstrapper',\n",
       " 'bounceexchange',\n",
       " 'br',\n",
       " 'branding',\n",
       " 'breakingnews',\n",
       " 'breakoutmedia',\n",
       " 'broadcasting',\n",
       " 'browsi',\n",
       " 'brsf',\n",
       " 'bt',\n",
       " 'btn',\n",
       " 'buckets',\n",
       " 'bue',\n",
       " 'bundles',\n",
       " 'buttonstyles',\n",
       " 'bw',\n",
       " 'bwzf',\n",
       " 'bxc',\n",
       " 'bxivhb',\n",
       " 'byline',\n",
       " 'bylinewrap',\n",
       " 'caj',\n",
       " 'calc',\n",
       " 'callout',\n",
       " 'cancelbutton',\n",
       " 'capitol',\n",
       " 'caps',\n",
       " 'cards',\n",
       " 'carousel',\n",
       " 'cars',\n",
       " 'casalemedia',\n",
       " 'caucuses',\n",
       " 'caused',\n",
       " 'cc',\n",
       " 'cd',\n",
       " 'cdn',\n",
       " 'cdnassetpath',\n",
       " 'cdnjs',\n",
       " 'celebrities',\n",
       " 'ch',\n",
       " 'changing',\n",
       " 'charset',\n",
       " 'chartbeat',\n",
       " 'checkbox',\n",
       " 'checking',\n",
       " 'checkmark',\n",
       " 'children',\n",
       " 'chk',\n",
       " 'chromeless',\n",
       " 'chunks',\n",
       " 'cid',\n",
       " 'cities',\n",
       " 'clicked',\n",
       " 'clipboard',\n",
       " 'cloudflare',\n",
       " 'cm',\n",
       " 'cn',\n",
       " 'cnn',\n",
       " 'cnnbiz',\n",
       " 'cnnbusiness',\n",
       " 'cnnclock',\n",
       " 'cnncreativemarketing',\n",
       " 'cnndotcom',\n",
       " 'cnnentertainment',\n",
       " 'cnnespanol',\n",
       " 'cnngo',\n",
       " 'cnni',\n",
       " 'cnninternational',\n",
       " 'cnniuk',\n",
       " 'cnnlabs',\n",
       " 'cnnmoney',\n",
       " 'cnnnewsletter',\n",
       " 'cnnnewsource',\n",
       " 'cnnnext',\n",
       " 'cnnpolitics',\n",
       " 'cnnsans',\n",
       " 'cnnstore',\n",
       " 'cnnstyle',\n",
       " 'cnntravel',\n",
       " 'co',\n",
       " 'cobranding',\n",
       " 'collapsed',\n",
       " 'columns',\n",
       " 'com',\n",
       " 'comments',\n",
       " 'condensedbold',\n",
       " 'condensedlight',\n",
       " 'condensedmedium',\n",
       " 'conditionalremove',\n",
       " 'conditions',\n",
       " 'const',\n",
       " 'containers',\n",
       " 'contentbox',\n",
       " 'contexts',\n",
       " 'controls',\n",
       " 'conviva',\n",
       " 'cookie',\n",
       " 'cookielaw',\n",
       " 'countdown',\n",
       " 'coupons',\n",
       " 'cp',\n",
       " 'criteo',\n",
       " 'crossorigin',\n",
       " 'css',\n",
       " 'csscolumns',\n",
       " 'csstransforms',\n",
       " 'cta',\n",
       " 'currencies',\n",
       " 'cvp',\n",
       " 'cvplive',\n",
       " 'cvpstream',\n",
       " 'cw',\n",
       " 'cygnus',\n",
       " 'darkprovider',\n",
       " 'darkprovidermessage',\n",
       " 'darkproviderokbutton',\n",
       " 'dataset',\n",
       " 'davos',\n",
       " 'db',\n",
       " 'dba',\n",
       " 'dc',\n",
       " 'dd',\n",
       " 'ddc',\n",
       " 'deactive',\n",
       " 'deals',\n",
       " 'debug',\n",
       " 'decisions',\n",
       " 'degrees',\n",
       " 'delays',\n",
       " 'delivered',\n",
       " 'delivering',\n",
       " 'demo',\n",
       " 'desc',\n",
       " 'desktop',\n",
       " 'destinations',\n",
       " 'details',\n",
       " 'detected',\n",
       " 'developing',\n",
       " 'dfdfdf',\n",
       " 'dfe',\n",
       " 'dfn',\n",
       " 'dialog',\n",
       " 'digits',\n",
       " 'dir',\n",
       " 'dj',\n",
       " 'dl',\n",
       " 'dmedianet',\n",
       " 'dnqmqq',\n",
       " 'dns',\n",
       " 'documentcloud',\n",
       " 'donald',\n",
       " 'dontseebutton',\n",
       " 'dots',\n",
       " 'download',\n",
       " 'dp',\n",
       " 'dr',\n",
       " 'drmgik',\n",
       " 'dropdown',\n",
       " 'dsum',\n",
       " 'duffels',\n",
       " 'dxr',\n",
       " 'editionized',\n",
       " 'editions',\n",
       " 'ee',\n",
       " 'eee',\n",
       " 'elections',\n",
       " 'elements',\n",
       " 'email',\n",
       " 'embedded',\n",
       " 'embeds',\n",
       " 'embedscripts',\n",
       " 'emitted',\n",
       " 'enabled',\n",
       " 'encountered',\n",
       " 'endif',\n",
       " 'ensighten',\n",
       " 'env',\n",
       " 'eot',\n",
       " 'eq',\n",
       " 'equiv',\n",
       " 'errormessage',\n",
       " 'errorokbutton',\n",
       " 'espanol',\n",
       " 'esports',\n",
       " 'etrain',\n",
       " 'europe',\n",
       " 'events',\n",
       " 'evolved',\n",
       " 'examines',\n",
       " 'exceeded',\n",
       " 'excercise',\n",
       " 'exe',\n",
       " 'exec',\n",
       " 'exlarge',\n",
       " 'expandable',\n",
       " 'expandfull',\n",
       " 'experiments',\n",
       " 'expired',\n",
       " 'expires',\n",
       " 'explains',\n",
       " 'ez',\n",
       " 'facebook',\n",
       " 'facts',\n",
       " 'fadeout',\n",
       " 'fafafa',\n",
       " 'fastforward',\n",
       " 'fastlane',\n",
       " 'fav',\n",
       " 'fave',\n",
       " 'favicon',\n",
       " 'favorites',\n",
       " 'fb',\n",
       " 'fbf',\n",
       " 'fbk',\n",
       " 'features',\n",
       " 'fefefe',\n",
       " 'ff',\n",
       " 'ffe',\n",
       " 'fff',\n",
       " 'fg',\n",
       " 'fields',\n",
       " 'fieldset',\n",
       " 'figcaption',\n",
       " 'files',\n",
       " 'findbyname',\n",
       " 'fivethings',\n",
       " 'fjdhp',\n",
       " 'fjs',\n",
       " 'flashplayer',\n",
       " 'flexbox',\n",
       " 'flyout',\n",
       " 'focusring',\n",
       " 'fontface',\n",
       " 'fonts',\n",
       " 'foodanddrink',\n",
       " 'footerstyles',\n",
       " 'forms',\n",
       " 'frameborder',\n",
       " 'fueled',\n",
       " 'fullscreen',\n",
       " 'fullstandardwidth',\n",
       " 'fullwidth',\n",
       " 'fw',\n",
       " 'fwmrm',\n",
       " 'fx',\n",
       " 'fyif',\n",
       " 'fyre',\n",
       " 'gadgets',\n",
       " 'games',\n",
       " 'gb',\n",
       " 'gc',\n",
       " 'geoslate',\n",
       " 'germany',\n",
       " 'gets',\n",
       " 'gigya',\n",
       " 'globetrotting',\n",
       " 'gm',\n",
       " 'goodstuff',\n",
       " 'google',\n",
       " 'googleadservices',\n",
       " 'googleplus',\n",
       " 'googlesyndication',\n",
       " 'googletagservices',\n",
       " 'gov',\n",
       " 'gpt',\n",
       " 'gqjm',\n",
       " 'graphql',\n",
       " 'grayscale',\n",
       " 'greatbigstorycnnnewsletter',\n",
       " 'gt',\n",
       " 'gz',\n",
       " 'has',\n",
       " 'hashtag',\n",
       " 'hdnea',\n",
       " 'headerless',\n",
       " 'headerstyles',\n",
       " 'heads',\n",
       " 'heavyit',\n",
       " 'heroes',\n",
       " 'hgroup',\n",
       " 'highlights',\n",
       " 'hln',\n",
       " 'homepage',\n",
       " 'homes',\n",
       " 'horseracing',\n",
       " 'hotels',\n",
       " 'hotlinks',\n",
       " 'hotstocks',\n",
       " 'hours',\n",
       " 'hpt',\n",
       " 'hr',\n",
       " 'href',\n",
       " 'hreflang',\n",
       " 'hrj',\n",
       " 'hsla',\n",
       " 'ht',\n",
       " 'html',\n",
       " 'hto',\n",
       " 'htp',\n",
       " 'http',\n",
       " 'https',\n",
       " 'humans',\n",
       " 'hy',\n",
       " 'iab',\n",
       " 'iabt',\n",
       " 'ib',\n",
       " 'ico',\n",
       " 'icons',\n",
       " 'ideas',\n",
       " 'ids',\n",
       " 'idx',\n",
       " 'iefix',\n",
       " 'iemobile',\n",
       " 'ieunsupported',\n",
       " 'iframe',\n",
       " 'iframeclient',\n",
       " 'ik',\n",
       " 'images',\n",
       " 'img',\n",
       " 'inbox',\n",
       " 'indexww',\n",
       " 'india',\n",
       " 'info',\n",
       " 'infographic',\n",
       " 'init',\n",
       " 'initials',\n",
       " 'inline',\n",
       " 'innercontainer',\n",
       " 'inpage',\n",
       " 'insights',\n",
       " 'instagram',\n",
       " 'instagramjs',\n",
       " 'instanceof',\n",
       " 'instructions',\n",
       " 'interviews',\n",
       " 'intl',\n",
       " 'investigates',\n",
       " 'investing',\n",
       " 'invoked',\n",
       " 'ios',\n",
       " 'ipad',\n",
       " 'iphone',\n",
       " 'ireport',\n",
       " 'isshorturl',\n",
       " 'issues',\n",
       " 'italic',\n",
       " 'itemprop',\n",
       " 'items',\n",
       " 'itemscope',\n",
       " 'itemtype',\n",
       " 'iws',\n",
       " 'jackets',\n",
       " 'jaf',\n",
       " 'javascript',\n",
       " 'jlz',\n",
       " 'jobs',\n",
       " 'jp',\n",
       " 'jpg',\n",
       " 'jplayer',\n",
       " 'jpsuperheaderwrapper',\n",
       " 'jr',\n",
       " 'js',\n",
       " 'jsmd',\n",
       " 'json',\n",
       " 'jsonp',\n",
       " 'jsp',\n",
       " 'jsx',\n",
       " 'jumbotron',\n",
       " 'kbd',\n",
       " 'kc',\n",
       " 'keyframes',\n",
       " 'keywords',\n",
       " 'khh',\n",
       " 'khtml',\n",
       " 'kk',\n",
       " 'krxd',\n",
       " 'ksljy',\n",
       " 'kv',\n",
       " 'kxpk',\n",
       " 'laa',\n",
       " 'lang',\n",
       " 'lastmod',\n",
       " 'launched',\n",
       " 'launches',\n",
       " 'launching',\n",
       " 'lawrence',\n",
       " 'lb',\n",
       " 'ld',\n",
       " 'leftarrow',\n",
       " 'legaldocs',\n",
       " 'lendingtree',\n",
       " 'lf',\n",
       " 'lh',\n",
       " 'lib',\n",
       " 'libs',\n",
       " 'lifestyle',\n",
       " 'liftoff',\n",
       " 'lightbox',\n",
       " 'lightit',\n",
       " 'liked',\n",
       " 'linkedin',\n",
       " 'linksstyles',\n",
       " 'listexpandable',\n",
       " 'lists',\n",
       " 'liveblog',\n",
       " 'livefyre',\n",
       " 'll',\n",
       " 'loc',\n",
       " 'localhost',\n",
       " 'logo',\n",
       " 'logocont',\n",
       " 'longform',\n",
       " 'lookup',\n",
       " 'lowercase',\n",
       " 'lowleft',\n",
       " 'lowright',\n",
       " 'lt',\n",
       " 'lte',\n",
       " 'ltr',\n",
       " 'mab',\n",
       " 'magellan',\n",
       " 'mailcar',\n",
       " 'mailchimp',\n",
       " 'maj',\n",
       " 'mapbox',\n",
       " 'maps',\n",
       " 'markets',\n",
       " 'max',\n",
       " 'maximized',\n",
       " 'mdash',\n",
       " 'means',\n",
       " 'medianet',\n",
       " 'mediumit',\n",
       " 'mega',\n",
       " 'meganav',\n",
       " 'members',\n",
       " 'metadata',\n",
       " 'microgravity',\n",
       " 'microsoft',\n",
       " 'middleeast',\n",
       " 'middycdn',\n",
       " 'midroll',\n",
       " 'miles',\n",
       " 'milliseconds',\n",
       " 'mini',\n",
       " 'minimap',\n",
       " 'minmax',\n",
       " 'minutes',\n",
       " 'misc',\n",
       " 'mlb',\n",
       " 'mms',\n",
       " 'modules',\n",
       " 'monitors',\n",
       " 'monospace',\n",
       " 'motorsport',\n",
       " 'movers',\n",
       " 'movies',\n",
       " 'moz',\n",
       " 'mp',\n",
       " 'ms',\n",
       " 'msa',\n",
       " 'msg',\n",
       " 'msib',\n",
       " 'msibh',\n",
       " 'mugs',\n",
       " 'multi',\n",
       " 'muted',\n",
       " 'mvpd',\n",
       " 'mvpdlist',\n",
       " 'mvpdlogo',\n",
       " 'mvpdpicker',\n",
       " 'mvpdsbylogo',\n",
       " 'mvpdsbyname',\n",
       " 'mvpdsearch',\n",
       " 'mvpdspinner',\n",
       " 'named',\n",
       " 'nasa',\n",
       " 'nav',\n",
       " 'navbar',\n",
       " 'nba',\n",
       " 'ncomments',\n",
       " 'newsletters',\n",
       " 'newsource',\n",
       " 'nfl',\n",
       " 'ng',\n",
       " 'nm',\n",
       " 'nocaps',\n",
       " 'noopener',\n",
       " 'noprovider',\n",
       " 'noprovidermessage',\n",
       " 'noproviderokbutton',\n",
       " 'noreferrer',\n",
       " 'northropgrumman',\n",
       " 'noscript',\n",
       " 'notfound',\n",
       " 'notifications',\n",
       " 'notifycount',\n",
       " 'notifycountvalue',\n",
       " 'nowrap',\n",
       " 'ntv',\n",
       " 'nyutn',\n",
       " 'ob',\n",
       " 'obct',\n",
       " 'obtp',\n",
       " 'occurred',\n",
       " 'ocs',\n",
       " 'offerings',\n",
       " 'offline',\n",
       " 'og',\n",
       " 'okbutton',\n",
       " 'ol',\n",
       " 'olympics',\n",
       " 'onboard',\n",
       " 'onboarding',\n",
       " 'onesignal',\n",
       " 'online',\n",
       " 'onload',\n",
       " 'onreadystatechange',\n",
       " 'opentype',\n",
       " 'opinions',\n",
       " 'optanon',\n",
       " 'optimized',\n",
       " 'optimizely',\n",
       " 'optimizelyjs',\n",
       " 'options',\n",
       " 'org',\n",
       " 'osx',\n",
       " 'ot',\n",
       " 'outbrain',\n",
       " 'outercontainer',\n",
       " 'pagead',\n",
       " 'pagebadge',\n",
       " 'pages',\n",
       " 'pagetop',\n",
       " 'paginated',\n",
       " 'parenting',\n",
       " 'partners',\n",
       " 'pconid',\n",
       " 'perf',\n",
       " 'persists',\n",
       " 'perspectives',\n",
       " 'pf',\n",
       " 'pg',\n",
       " 'photos',\n",
       " 'pickbylogo',\n",
       " 'pingforpull',\n",
       " 'pinterest',\n",
       " 'pk',\n",
       " 'placeholder',\n",
       " 'planning',\n",
       " 'playername',\n",
       " 'playing',\n",
       " 'playlist',\n",
       " 'playlists',\n",
       " 'pmd',\n",
       " 'png',\n",
       " 'politicsapp',\n",
       " 'poptip',\n",
       " 'postroll',\n",
       " 'posts',\n",
       " 'pounds',\n",
       " 'poweredby',\n",
       " 'pre',\n",
       " 'preferences',\n",
       " 'prefetch',\n",
       " 'preload',\n",
       " 'premarket',\n",
       " 'premarkets',\n",
       " 'preroll',\n",
       " 'prev',\n",
       " 'primaries',\n",
       " 'problems',\n",
       " 'profiles',\n",
       " 'promo',\n",
       " 'proximic',\n",
       " 'pts',\n",
       " 'pubdate',\n",
       " 'publishertag',\n",
       " 'pullquote',\n",
       " 'py',\n",
       " 'pz',\n",
       " 'pzv',\n",
       " 'quotes',\n",
       " 'rdx',\n",
       " 'reactangle',\n",
       " 'readers',\n",
       " 'rebelmouse',\n",
       " 'rec',\n",
       " 'recommended',\n",
       " 'redalert',\n",
       " 'reddit',\n",
       " 'refdom',\n",
       " 'refreshed',\n",
       " 'regions',\n",
       " 'released',\n",
       " 'reliablesources',\n",
       " 'remembered',\n",
       " 'rememberedcancelbutton',\n",
       " 'rememberedmessage',\n",
       " 'rememberedokbutton',\n",
       " 'rememberedprovider',\n",
       " 'rendered',\n",
       " 'reporters',\n",
       " 'reqfields',\n",
       " 'required',\n",
       " 'results',\n",
       " 'rgba',\n",
       " 'rightarrow',\n",
       " 'robert',\n",
       " 'robotic',\n",
       " 'rollups',\n",
       " 'rows',\n",
       " 'rss',\n",
       " 'rtax',\n",
       " 'rtl',\n",
       " 'rubiconproject',\n",
       " 'rugby',\n",
       " 'sc',\n",
       " 'scheduled',\n",
       " 'scotus',\n",
       " 'scripts',\n",
       " 'scripttemplates',\n",
       " 'scrollbar',\n",
       " 'scrolling',\n",
       " 'scrollover',\n",
       " 'sdks',\n",
       " 'searchapp',\n",
       " 'searchfield',\n",
       " 'searchheader',\n",
       " 'searchpane',\n",
       " 'sections',\n",
       " 'semibold',\n",
       " 'services',\n",
       " 'settings',\n",
       " 'sharebar',\n",
       " 'shared',\n",
       " 'showbiz',\n",
       " 'showhide',\n",
       " 'shows',\n",
       " 'siblings',\n",
       " 'sidenotes',\n",
       " 'signin',\n",
       " 'signincancelbutton',\n",
       " 'signinmessage',\n",
       " 'signs',\n",
       " 'sitemap',\n",
       " 'sites',\n",
       " 'slates',\n",
       " 'slidename',\n",
       " 'smartest',\n",
       " 'smv',\n",
       " 'software',\n",
       " 'solutions',\n",
       " 'sourced',\n",
       " 'sources',\n",
       " 'spacecraft',\n",
       " 'spanish',\n",
       " 'specials',\n",
       " 'spinnerbox',\n",
       " 'spon',\n",
       " 'sponsored',\n",
       " 'spreecast',\n",
       " 'spt',\n",
       " 'src',\n",
       " 'ssl',\n",
       " 'staggered',\n",
       " 'stars',\n",
       " 'steps',\n",
       " 'stored',\n",
       " 'stories',\n",
       " 'storifycon',\n",
       " 'storyurl',\n",
       " 'stripes',\n",
       " 'strongest',\n",
       " 'studentnews',\n",
       " 'studios',\n",
       " 'stumbleupon',\n",
       " 'styled',\n",
       " 'styles',\n",
       " 'subhub',\n",
       " 'submitted',\n",
       " 'subnav',\n",
       " 'subpixel',\n",
       " 'subs',\n",
       " 'subscribers',\n",
       " 'subsections',\n",
       " 'successmessage',\n",
       " 'supplies',\n",
       " 'supported',\n",
       " 'svg',\n",
       " 'swf',\n",
       " 'switzerland',\n",
       " 'synced',\n",
       " 'systeinc',\n",
       " 'tabindex',\n",
       " 'tags',\n",
       " 'takers',\n",
       " 'takes',\n",
       " 'targeting',\n",
       " 'teaseimage',\n",
       " 'temppass',\n",
       " 'terms',\n",
       " 'terra',\n",
       " 'textarea',\n",
       " 'textfield',\n",
       " 'textshadow',\n",
       " 'theo',\n",
       " 'theoplayer',\n",
       " 'thinit',\n",
       " 'thoughts',\n",
       " 'thousands',\n",
       " 'timeout',\n",
       " 'timestamp',\n",
       " 'tn',\n",
       " 'toggles',\n",
       " 'tools',\n",
       " 'tooltip',\n",
       " 'tooltips',\n",
       " 'topics',\n",
       " 'topvideos',\n",
       " 'tos',\n",
       " 'touchevents',\n",
       " 'touchhover',\n",
       " 'tours',\n",
       " 'tpc',\n",
       " 'tracking',\n",
       " 'transcripts',\n",
       " 'transformed',\n",
       " 'tripadvisor',\n",
       " 'trnd',\n",
       " 'troubleshooting',\n",
       " 'tru',\n",
       " 'truetype',\n",
       " 'trumpmerica',\n",
       " 'truste',\n",
       " 'tt',\n",
       " 'ttf',\n",
       " 'ttl',\n",
       " 'tumblr',\n",
       " 'turnerjobs',\n",
       " 'tv',\n",
       " 'tve',\n",
       " 'tvem',\n",
       " 'tvschedule',\n",
       " 'tvtag',\n",
       " 'tw',\n",
       " 'twembed',\n",
       " 'twitterjs',\n",
       " 'twocol',\n",
       " 'twq',\n",
       " 'twsrc',\n",
       " 'typeahead',\n",
       " 'typeof',\n",
       " 'uc',\n",
       " 'uetq',\n",
       " 'ugdturner',\n",
       " 'ui',\n",
       " 'uid',\n",
       " 'uk',\n",
       " 'ul',\n",
       " 'underscored',\n",
       " 'unfolds',\n",
       " 'unfurled',\n",
       " 'unhackable',\n",
       " 'units',\n",
       " 'unmute',\n",
       " 'unsupp',\n",
       " 'updated',\n",
       " 'updates',\n",
       " 'upload',\n",
       " 'uppercase',\n",
       " 'upstarts',\n",
       " 'uri',\n",
       " 'uris',\n",
       " 'url',\n",
       " 'usabilla',\n",
       " 'username',\n",
       " 'using',\n",
       " 'usr',\n",
       " 'utf',\n",
       " 'utilities',\n",
       " 'utm',\n",
       " 'uwt',\n",
       " 'validated',\n",
       " 'values',\n",
       " 'var',\n",
       " 've',\n",
       " 'verticals',\n",
       " 'vgtf',\n",
       " 'videoapi',\n",
       " 'videographers',\n",
       " 'videolanding',\n",
       " 'videos',\n",
       " 'videx',\n",
       " 'viewallbutton',\n",
       " 'viewport',\n",
       " 'views',\n",
       " 'viewtopbutton',\n",
       " 'vimeo',\n",
       " 'visited',\n",
       " 'vjs',\n",
       " 'vod',\n",
       " 'votes',\n",
       " 'vpaid',\n",
       " 'vr',\n",
       " 'vrt',\n",
       " 'watchcnn',\n",
       " 'watchcnnlive',\n",
       " 'watchnowbutton',\n",
       " 'webkit',\n",
       " 'webservices',\n",
       " 'website',\n",
       " 'webview',\n",
       " 'welcomeloginbutton',\n",
       " 'welcomemessage',\n",
       " 'whatsapp',\n",
       " 'widget',\n",
       " 'widgets',\n",
       " 'windows',\n",
       " 'winningpost',\n",
       " 'woff',\n",
       " 'worldsport',\n",
       " 'wp',\n",
       " 'www',\n",
       " 'xfbml',\n",
       " 'xlarge',\n",
       " 'xmlns',\n",
       " 'xs',\n",
       " 'xsmall',\n",
       " 'xtype',\n",
       " 'yieldmo',\n",
       " 'youtube',\n",
       " 'zion',\n",
       " 'zn',\n",
       " 'zqtk']"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown(\"https://www.cnn.com/2020/02/15/us/nasa-launch-experiments-supplies-space-station-trnd/index.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define $\\mu_w$ to be the average number of letters per word, and $\\mu_s$ to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: $$4.71 \\mu w + 0.5 \\mu s - 21.43$$. Write a function `readability` that computes the ARI score for various sections of the Brown Corpus. The function should take as an arugment a section of the Brown Corpus represented by a letter (e.g. Section F is \"popular lore\" and Section J is \"learned\"). Make use of the fact that `nltk.corpus.brown.words()` produces a sequence of words, while `nltk.corpus.brown.sents()` produces a sequence of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.254756197101155"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readability(\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Execute the following <u>two cells</u> which specifies some unit tests for the above problems:\n",
    "* The first cell specifies the tests.\n",
    "* The second cell runs the tests.\n",
    "\n",
    "I will test your code using these, as well as other held-out tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestMethods(unittest.TestCase):\n",
    "\n",
    "    def test_is_palindrome(self):\n",
    "        self.assertTrue(is_palindrome(\"radar\"))\n",
    "        self.assertFalse(is_palindrome(\"radars\"))\n",
    "\n",
    "    def test_odd_list(self):\n",
    "        self.assertEqual(odd_list([1,2,3,4,5]), [1,3,5])\n",
    "\n",
    "    def test_is_sorted(self):\n",
    "        self.assertTrue(is_sorted([1,2,2]))\n",
    "        self.assertFalse(is_sorted(['b','a']))\n",
    "        \n",
    "    def test_starts_with_vowel(self):\n",
    "        self.assertTrue(starts_with_vowel(\"a\"))\n",
    "        self.assertTrue(starts_with_vowel(\"U\"))        \n",
    "\n",
    "    def test_is_anagram(self):\n",
    "        self.assertTrue(is_anagram(\"school master\", \"the classroom\"))\n",
    "        self.assertFalse(is_anagram(\"school\", \"classroom\"))\n",
    "\n",
    "    def test_is_pangram(self):\n",
    "        self.assertTrue(is_pangram(\"the quick brown fox jumps over the lazy dog\"))\n",
    "        self.assertFalse(is_pangram(\"the quick brown fox jumps over the lazy fox\"))\n",
    "\n",
    "    def test_char_freq(self):\n",
    "        self.assertEqual(char_freq('to be or not to be'),\n",
    "                         {'t':3, 'o':4, ' ':5, 'b':2, 'e':2, 'r':1, 'n':1})\n",
    "\n",
    "    def test_encode(self):\n",
    "        self.assertEqual(encode(\"To be or not to be, That is the question\"), \"Gb or be abg gb or, Gung vf gur dhrfgvba\")\n",
    "\n",
    "    def test_decode(self):\n",
    "        self.assertEqual(decode(\"Gb or be abg gb or, Gung vf gur dhrfgvba\"), \"To be or not to be, That is the question\")\n",
    "\n",
    "    def test_word_length(self):\n",
    "        self.assertEqual(word_length([\"the\", \"quick\", \"brown\", \"fox\"]), [3, 5, 5, 3])\n",
    "\n",
    "    def test_filter_long_words(self):\n",
    "        self.assertEqual(filter_long_words([\"the\", \"quick\", \"brown\", \"fox\"], 4), [\"QUICK\", \"BROWN\"])\n",
    "        \n",
    "    def test_password_check(self):\n",
    "        self.assertFalse(password_check('password'))          \n",
    "        self.assertTrue(password_check('pAs$w0rd'))\n",
    "\n",
    "    def test_b_alphabetical(self):\n",
    "        self.assertEqual(b_alphabetical(['baseball','is','not','boring']), ['baseball','boring'])\n",
    "\n",
    "    def test_text_select(self):\n",
    "        self.assertEqual(text_select(nltk.Text(['the','cat','Axptxize','in'])), ['Axptxize'])\n",
    "        self.assertEqual(text_select(nltk.Text(['the','cat','in'])), [])\n",
    "        self.assertEqual(text_select(nltk.Text(['the','Pdgspsdptize','Axptxize','in'])), \n",
    "            ['Pdgspsdptize','Axptxize'])\n",
    "\n",
    "    def test_vocab_size(self):\n",
    "        self.assertEqual(vocab_size(nltk.book.text4), 9754)  \n",
    "        self.assertEqual(vocab_size(nltk.Text(['the','cat','in','the','hat'])), 4)\n",
    "    \n",
    "    def test_state_union(self):\n",
    "        dict = state_union()\n",
    "        self.assertEqual(dict['2006-GWBush.txt'], (7,7,22))\n",
    "\n",
    "    def test_fifty(self):\n",
    "        self.assertEqual(fifty(nltk.book.text7)[:5], [',', '.', '*-1', '0', '*'])\n",
    "        self.assertEqual(fifty(nltk.book.text7)[5:9], [\"'s\", '*T*-1', '*U*', '$'])\n",
    "        self.assertTrue('million' in fifty(nltk.book.text7))\n",
    "        self.assertTrue('said' in fifty(nltk.book.text7))\n",
    "        self.assertFalse('he' in fifty(nltk.book.text7))\n",
    "        self.assertFalse('deposit' in fifty(nltk.book.text7))\n",
    "\n",
    "\n",
    "    def test_novel_10(self):\n",
    "        self.assertEqual(novel_10(nltk.Text(['the','cat','in','the','hat','ate','the','big','hairy','and','gray','rat','hat'])), ['rat'])\n",
    "\n",
    "    # TODO -- fix this test\n",
    "    def test_sent_info(self):\n",
    "        self.assertEqual(sent_info('the cat in the hat'),\n",
    "            \"\"\"cat 1\n",
    "hat 1\n",
    "in 1\n",
    "the 2\"\"\")\n",
    "\n",
    "    \n",
    "    def test_unknown(self):\n",
    "        #unknownlist = unknown('http://www.cs.wcupa.edu/rburns/NLP')\n",
    "        unknownlist = unknown('http://www.google.com')\n",
    "        self.assertFalse('event' in unknownlist)\n",
    "        self.assertFalse('credit' in unknownlist)\n",
    "        self.assertTrue('google' in unknownlist)\n",
    "\n",
    "    def test_readability(self):\n",
    "        self.assertAlmostEqual(readability('f'), 10.254756197101155, places=2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_b_alphabetical (__main__.TestMethods) ... ok\n",
      "test_char_freq (__main__.TestMethods) ... ok\n",
      "test_decode (__main__.TestMethods) ... ok\n",
      "test_encode (__main__.TestMethods) ... ok\n",
      "test_fifty (__main__.TestMethods) ... ok\n",
      "test_filter_long_words (__main__.TestMethods) ... ok\n",
      "test_is_anagram (__main__.TestMethods) ... ok\n",
      "test_is_palindrome (__main__.TestMethods) ... ok\n",
      "test_is_pangram (__main__.TestMethods) ... ok\n",
      "test_is_sorted (__main__.TestMethods) ... ok\n",
      "test_novel_10 (__main__.TestMethods) ... ok\n",
      "test_odd_list (__main__.TestMethods) ... ok\n",
      "test_password_check (__main__.TestMethods) ... ok\n",
      "test_readability (__main__.TestMethods) ... ok\n",
      "test_sent_info (__main__.TestMethods) ... FAIL\n",
      "test_starts_with_vowel (__main__.TestMethods) ... ok\n",
      "test_state_union (__main__.TestMethods) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat 1\n",
      "hat 1\n",
      "in 1\n",
      "the 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_text_select (__main__.TestMethods) ... ok\n",
      "test_unknown (__main__.TestMethods) ... ok\n",
      "test_vocab_size (__main__.TestMethods) ... FAIL\n",
      "test_word_length (__main__.TestMethods) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_sent_info (__main__.TestMethods)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-682-d29374e0ab17>\", line 83, in test_sent_info\n",
      "    the 2\"\"\")\n",
      "AssertionError: None != 'cat 1\\nhat 1\\nin 1\\nthe 2'\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_vocab_size (__main__.TestMethods)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-682-d29374e0ab17>\", line 58, in test_vocab_size\n",
      "    self.assertEqual(vocab_size(nltk.book.text4), 9754)\n",
      "AssertionError: 9792 != 9754\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 21 tests in 2.781s\n",
      "\n",
      "FAILED (failures=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fb0d7bf1438>"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
