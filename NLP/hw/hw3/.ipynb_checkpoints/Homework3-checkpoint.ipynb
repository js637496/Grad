{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: Irish or Aussie?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Out: Thursday, March 5\n",
    "## Due Date: Tuesday, March 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This programming assignment aims to help you prepare for the final project in this course. It tasks you to:\n",
    "\n",
    "* download two large text datasets\n",
    "* pre-process them into a common format\n",
    "* divide them into appropriate training and testing sets\n",
    "* learn a naive bayes classifier on the training set\n",
    "* iterate and refine the model using a dev-test set\n",
    "* give a final evaluation of the model using a test set\n",
    "\n",
    "<u>You may work in teams of two or three (2-tuples or 3-tuples?) for this assignment.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This _never-seen-before_ assignment was motivated by our recent discussion of the NLTK Names Corpus and Jurafsky and Martin's example of a toy-classifier to distinguish between \"Japan\" and \"Chinese\".\n",
    "\n",
    "It was a bit difficult finding datasets of the size and detail that I wanted to assign.\n",
    "\n",
    "Before I settled on the classification task in this assignment __\"Irish or Aussie? (You will train a model that distinguishes between newspaper headlines from an Austrailian newspaper compared to an Irish newspaper.)__, I attempted to find good datasets on:\n",
    "* Jeopardy puzzle clues vs. Who Wants to Be a Millionaire clues\n",
    "* Peanuts comic strip text vs. Garfield\n",
    "* Seinfeld sitcom text vs. Friends\n",
    "* Wine reviews vs. Beer reviews\n",
    "* ...\n",
    "\n",
    "If you want, feel free to use different, but comparable datasets for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Download the Aussie and Irish headline datasets.</u>\n",
    "\n",
    "I found the datasets here:\n",
    "https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research\n",
    "\n",
    "Look at the \"text data\" section. There, you'll see citation hyperlinks 168 and 170, which will take you to a kaggle page where you can download the data:\n",
    "1. A Million News Headlines (these are the headlines for Australia -- Aussie)\n",
    "2. The Irish Times (Irish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sets downloaded and in same directory as notebook.  Renamed aussie.csv and irish.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Process the downloaded <code>csv</code> files in python.</u>\n",
    "\n",
    "Note that the format in the two datasets are different. Throw out any columns that you don't need. \n",
    "\n",
    "Perhaps normalize the data and lowercase everything. Observe that one dataset has capitalization differences. \n",
    "\n",
    "Transform the data into some python data structure that also captures whether the headline is Irish or Aussie. I recommend reviewing how we used the tuple `(instance, class)` representation when analyzing the Names Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irish contains punctuation and capital letters\n",
    "# aussie is all lower case with no punctutation\n",
    "# aussie - date,headline\n",
    "# irish - date,news label,headline\n",
    "# Use the head.csv file in each folder for testing\n",
    "# Both csv files are comma deliminated\n",
    "\n",
    "#Field limit\n",
    "import sys\n",
    "import csv\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        \n",
    "def readInCSV(thecsv, rawlist, colname):\n",
    "    with thecsv as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file, delimiter=\",\")\n",
    "        for lines in csv_reader:\n",
    "            rawlist.append(lines[colname])\n",
    "\n",
    "    \n",
    "irishcsv = open(\"irish.csv\", \"r\")\n",
    "aussiecsv = open(\"aussie.csv\", \"r\")\n",
    "irish = []\n",
    "aussie = []\n",
    "\n",
    "readInCSV(irishcsv, irish, \"headline_text\") \n",
    "readInCSV(aussiecsv, aussie, \"headline_text\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the text\n",
    "badchars = [\"'\", \"’\", \"”\", \"“\", \";\", \".\", \"!\", \"?\", \"-\", \":\", \"%\", \")\", \"(\", \"_\"]\n",
    "def normalizeText(lines):\n",
    "    for i, line in enumerate(lines):\n",
    "        lines[i] = (''.join(i for i in line if not i in badchars)).lower()\n",
    "\n",
    "normalizeText(aussie)\n",
    "normalizeText(irish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are an unequal number of headlines.\n",
    "# Irish - 1425460\n",
    "# Aussie - 1186018\n",
    "# Randomly select 1186018 headlines to keep so we can create equally wieghted training sets\n",
    "\n",
    "import random\n",
    "\n",
    "irish = random.sample(irish, 1186018)\n",
    "\n",
    "# Shuffle both lists\n",
    "random.shuffle(irish)\n",
    "random.shuffle(aussie)\n",
    "\n",
    "w = csv.writer(open(\"masterset.csv\", \"w\"))\n",
    "for i in range(len(irish)):\n",
    "    w.writerow([irish[i], \"irish\"])\n",
    "    w.writerow([aussie[i], \"aussie\"])\n",
    "\n",
    "#create a master training csv that we can use to split into the training, dev testing and testing sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Divide your data into a training set and testing set.</u>\n",
    "\n",
    "No peaking at the test set here on out!\n",
    "\n",
    "Perhaps even divide your training set into a dev-test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split master.csv into 3 equal sets: training, dev and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "master = open(\"masterset.csv\", \"r\").readlines()\n",
    "splitInc = math.floor(len(master) / 3)\n",
    "filename = [\"train\", \"dev\", \"test\"]\n",
    "fc = 0\n",
    "for i in range(len(master)):\n",
    "    if i % splitInc == 0 and fc < 3:\n",
    "        imax = i+splitInc\n",
    "        open(filename[fc] + '.csv', 'w').writelines(master[i:imax])\n",
    "        fc += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Learn a naive bayes classifier on the training set.</u>\n",
    "\n",
    "(For this step you may find it easier to deviate from the Names corpus classifier example, and instead manually compute probabilities following the intuition presented in the \"China\"/\"Japan\" J&M example.)\n",
    "\n",
    "All we really need here are: (1) prior probabilities and (2) conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv('train.csv', delimiter=',')\n",
    "#book says to use the entire set as the basis for all_words list\n",
    "#sounds like cheating\n",
    "#df = pd.read_csv('masterset.csv', delimiter=',')\n",
    "list_of_tuples = [tuple(row) for row in df.values]\n",
    "\n",
    "words = []\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for sHeadling in list_of_tuples :\n",
    "    words += nltk.word_tokenize(sHeadling[0])\n",
    "    \n",
    "filtered_words = [w for w in words if not w in stop_words] \n",
    "\n",
    "all_words = nltk.FreqDist(filtered_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(w for (w, x) in all_words.most_common(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(nltk.word_tokenize(document))\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing has shown that the more words we invlude in the word_feature list, the more accurate the model is.\n",
    "#Increasing the number of words in the word_features list will increase ram utilization\n",
    "\n",
    "import pickle\n",
    "featuresets = [(document_features(d), c) for (d,c) in list_of_tuples[:20000]]\n",
    "classifier = nltk.NaiveBayesClassifier.train(featuresets)\n",
    "\n",
    "save_classifier = open(\"naivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "classifier_f = open(\"naivebayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('dev.csv', delimiter=',')\n",
    "list_of_tuples = [tuple(row) for row in df.values]\n",
    "featuresets = [(document_features(d), c) for (d,c) in list_of_tuples[3000:3500]]\n",
    "print(nltk.classify.accuracy(classifier, featuresets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Iterate and refine the model using a dev-test set.</u>\n",
    "                        \n",
    "Let's try to make our model better. What instances is your model misclassifying? Report lessons learned either here, or at the bottom of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Evaluate your model on the held out test set.</u>\n",
    "                        \n",
    "Which metric is most appropriate? Accuracy?\n",
    "\n",
    "Is there anything else that could be useful to calculate? \n",
    "\n",
    "What is the classifier's baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a technical report (in this Jupyter Notebook, with good Markdown formatting) that documents your findings, \"lessons learned\", any areas of where you ran into difficult, and also any other interesting details. Include in your report the following details.\n",
    "\n",
    "The format of the report is up to you, and you may include this information either above, or right here. Use any appropriate layout that best conveys your narrative.\n",
    "\n",
    "Also submit this python notebook `.ipynb` to D2L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON CODE AND REPORT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
