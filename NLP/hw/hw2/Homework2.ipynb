{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: N-gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Out: Thursday, February 20\n",
    "## Due Date: Thursday, March 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This programming assignment is more open-ended than the previous ones. It is centered on the N-gram language models and tasks you to:\n",
    "\n",
    "* download and process a large text dataset in python using the <code>csv</code> library\n",
    "* perform sentence and word tokenization\n",
    "* calculate N-gram counts and probabilities\n",
    "* compare the characteristics of the N-grams across different models\n",
    "* generate random sentences using the models\n",
    "\n",
    "<u>You may work in teams of two or three (2-tuples or 3-tuples?) for this assignment.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Download two large text datasets from Kaggle.</u>\n",
    "\n",
    "The <a href=\"http://kaggle.com\">Kaggle competition hosting site</a> offers a number of free datasets that contain interesting text fields. For this assignment, we will use the \"Wine Reviews\" and \"All the News\" datasets. They can be accessed by selecting the \"Datasets\" header and then searching for these specific datasets. Then, choose \"Data\" from the sub-header, preview some of the csv data and notice how at least one of the columns in the dataset will contain sufficient text. I chose to direct you to these two datasets because the textual content seemed interesting and would have different language characteristics, and both were large csv files that could generate significant n-gram counts, but not be too large of a file.\n",
    "\n",
    "<em>(You can use other datasets if you wish. Others that looked interesting on Kaggle include the \"Yelp Dataset\" (but its over 3GB !!!), \"SMS Spam Collection Dataset\", \"Russian Troll Tweets\", and \"A Million News Headlines\".)</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloaded wine-reviews and all-the-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Process the downloaded <code>csv</code> files in python.</u>\n",
    "\n",
    "There's a nice csv library already included in python for accessing values in that are stored in a comma separated values (csv) format. Read the <a href=\"https://docs.python.org/3/library/csv.html\">csv library documentation</a>.\n",
    "What is the delimiter in your csv files? Open each of the two .csv files that you downloaded using this library and be able to read in the data. Note that we really only care about the text column in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the head.csv file in each folder for testing\n",
    "# Both csv files are comma deliminated\n",
    "\n",
    "#Field limit\n",
    "import sys\n",
    "import csv\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "\n",
    "#news = open(\"all-the-news/articles1.csv\", \"r\")\n",
    "#wine = open(\"wine-reviews/winemag-data-130k-v2.csv\", \"r\")\n",
    "#news = open(\"all-the-news/head1000.csv\", \"r\")\n",
    "#wine = open(\"wine-reviews/head1000.csv\", \"r\")\n",
    "#news = open(\"all-the-news/head500.csv\", \"r\")\n",
    "#wine = open(\"wine-reviews/head500.csv\", \"r\")\n",
    "#news = open(\"all-the-news/head50.csv\", \"r\")\n",
    "#wine = open(\"wine-reviews/head50.csv\", \"r\")\n",
    "news = open(\"all-the-news/head.csv\", \"r\")\n",
    "wine = open(\"wine-reviews/head.csv\", \"r\")\n",
    "\n",
    "reviews = []\n",
    "articles = []\n",
    "with news as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=\",\")\n",
    "    for lines in csv_reader:\n",
    "        articles.append(lines[\"content\"])\n",
    "with wine as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=\",\")\n",
    "    for lines in csv_reader:\n",
    "        reviews.append(lines[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(reviews)\n",
    "#print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Perform sentence segmentation and word tokenization.</u>\n",
    "\n",
    "Utilize the nltk module to perform sentence segmentation and word tokenization. But at this point, there are a few decisions that need to be made:\n",
    "\n",
    "* How we should handle the .csv rows in the previous step? If we ignore row makers, and \"lump everything together\", how will that effect our language model?\n",
    "* Do we want to remove punctuation? What is the effect of keeping punctuation in the model?\n",
    "* Do we want to add sentence boundary markers, such as <samp>&lt;S&gt;</samp> and <samp>&lt;/S&gt;</samp>?</li>\n",
    "* Should two the words <samp>The</samp> and <samp>the</samp> be treated as the same? What are the effects of doing, or not doing, this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READ REVIEWS\n",
      "READ REVIEWS COMPLETE\n",
      "READ NEWS\n",
      "READ NEWS COMPLETE\n",
      "SENT TOKE REVIEWS\n",
      "2020-02-27 17:50:06\n",
      "SENT TOKE REVIEWS COMPLETE\n",
      "2020-02-27 17:50:06\n",
      "SENT TOKE NEWS\n",
      "2020-02-27 17:50:06\n",
      "SENT TOKE NEWS COMPLETE\n",
      "2020-02-27 17:50:06\n"
     ]
    }
   ],
   "source": [
    "# Rows will be lumped into a single string.  A single \" \" will be added to the end of each row to ensure sentences\n",
    "# are not being combined (Ex. \"lastword. firstword\" instead of \"lastword.firstword\")\n",
    "# Punctuation will NOT be kept when counting n-grams\n",
    "# The and the will be treated as the same word, this will decrease total n-grams\n",
    "# N-Gram words will be counted within sentence boundries  (a trigram/bigram will not overlap into another sentence)\n",
    "import string\n",
    "import datetime\n",
    "\n",
    "reviewsraw = \"\"\n",
    "articlesraw = \"\"\n",
    "print(\"READ REVIEWS\")\n",
    "for review in reviews:\n",
    "    #review = review.translate(str.maketrans('', '', string.punctuation))\n",
    "    review = review.lower()\n",
    "    reviewsraw += review + \" \"\n",
    "print(\"READ REVIEWS COMPLETE\")\n",
    "print(\"READ NEWS\")\n",
    "for article in articles:\n",
    "    #article = article.translate(str.maketrans('', '', string.punctuation))\n",
    "    article = article.lower()\n",
    "    articlesraw += article + \" \"\n",
    "print(\"READ NEWS COMPLETE\")\n",
    "    \n",
    "#reviewTokens = nltk.word_tokenize(reviewsraw)\n",
    "print(\"SENT TOKE REVIEWS\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "reviewSents = nltk.sent_tokenize(reviewsraw)\n",
    "print(\"SENT TOKE REVIEWS COMPLETE\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(\"SENT TOKE NEWS\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "#articleTokens = nltk.word_tokenize(articlesraw)\n",
    "articleSents = nltk.sent_tokenize(articlesraw)\n",
    "print(\"SENT TOKE NEWS COMPLETE\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(reviewTokens)\n",
    "#print(reviewSents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Calculate N-gram counts and compute probabilities.</u>\n",
    "\n",
    "Use a python dictionary (or any suitable data structure) to first compute unigram counts. Then try bigram counts. Finally, trigram counts.\n",
    "\n",
    "How much memory are you using? How fast, or slow, is the code -- how long is this step taking? If it is taking too long, try only using a fraction of your corpus: instead of loading the entire .csv file, try only reading the first 1000 rows of data.\n",
    "\n",
    "Using those counts, compute the probabilities for the unigrams, bigrams, and trigrams, and store those in a new python dictionary (or some other data structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START WINE REVIEWS\n",
      "CREATING NGRAM COUNTS - WINE REIVEWS\n",
      "2020-02-27 17:51:54\n",
      "Review sentence length: 20\n",
      "Starting thread 0\n",
      "Starting thread 1\n",
      "Starting thread 2\n",
      "Starting thread 3\n",
      "Starting thread 4\n",
      "Thread 0: 0 of 3 2020-02-27 17:51:54\n",
      "Starting thread 5\n",
      "Starting thread 6\n",
      "Starting thread 7\n",
      "Thread 2: 0 of 3 2020-02-27 17:51:54\n",
      "Thread 1: 0 of 3 2020-02-27 17:51:54\n",
      "Thread 3: 0 of 3 2020-02-27 17:51:54\n",
      "Thread 5: 0 of 3 2020-02-27 17:51:54\n",
      "End thread 0\n",
      "Thread 4: 0 of 3 2020-02-27 17:51:54\n",
      "Thread 6: 0 of 2 2020-02-27 17:51:54\n",
      "End thread 1\n",
      "End thread 2\n",
      "End thread 3\n",
      "End thread 4\n",
      "End thread 5\n",
      "End thread 6\n",
      "End thread 7\n",
      "All threads complete.\n",
      "2020-02-27 17:51:54\n",
      "CREATING COUNTER DICTIONARY - WINE REIVEWS\n",
      "PROCCESSING UNIGRAMS PERCENTAGE - WINE REVIEWS\n",
      "PROCCESSING BIGRAM PERCENTAGE - WINE REVIEWS\n",
      "PROCCESSING TRIGRAMS PERCENTAGE - WINE REVIEWS\n",
      "SORTING WINE UNIGRAM PERCENT\n",
      "SORTING WINE UNIGRAM COUNT\n",
      "SORTING WINE BIGRAM PERCENT\n",
      "SORTING WINE BIGRAM COUNT\n",
      "SORTING WINE TRIGRAM PERCENT\n",
      "SORTING WINE TRIGRAM COUNT\n",
      "WRITING WINE FILES\n",
      "WRITING WINE unigramCounts\n",
      "WRITING WINE unigramPercent\n",
      "WRITING WINE bigramCounts\n",
      "WRITING WINE bigramPercent\n",
      "WRITING WINE trigramCounts\n",
      "WRITING WINE trigramPercent\n",
      "END WINE REVIEWS\n",
      "2020-02-27 17:51:54\n",
      "START NEWS\n",
      "CREATING NGRAM COUNTS - NEWS\n",
      "2020-02-27 17:51:54\n",
      "News sentence length: 20\n",
      "Starting thread 0\n",
      "Starting thread 1\n",
      "Starting thread 2\n",
      "Starting thread 3\n",
      "Starting thread 4\n",
      "Starting thread 5\n",
      "Thread 0: 0 of 90 2020-02-27 17:51:54\n",
      "Starting thread 6\n",
      "Starting thread 7\n",
      "Thread 1: 0 of 90 2020-02-27 17:51:54\n",
      "Thread 2: 0 of 90 2020-02-27 17:51:54\n",
      "Thread 3: 0 of 90 2020-02-27 17:51:54\n",
      "Thread 6: 0 of 90 2020-02-27 17:51:54\n",
      "Thread 4: 0 of 90 2020-02-27 17:51:54\n",
      "Thread 5: 0 of 90 2020-02-27 17:51:54\n",
      "Thread 7: 0 of 85 2020-02-27 17:51:54\n",
      "End thread 0\n",
      "End thread 1\n",
      "End thread 2\n",
      "End thread 3\n",
      "End thread 4\n",
      "End thread 5\n",
      "End thread 6\n",
      "End thread 7\n",
      "All threads complete\n",
      "2020-02-27 17:51:55\n",
      "CREATING COUNTER DICTIONARY - NEWS\n",
      "PROCCESSING UNIGRAMS PERCENTAGE - NEWS\n",
      "PROCCESSING BIGRAM PERCENTAGE - NEWS\n",
      "PROCCESSING TRIGRAM PERCENTAGE - NEWS\n",
      "SORTING NEWS UNIGRAM PERCENT\n",
      "SORTING NEWS UNIGRAM COUNT\n",
      "SORTING NEWS BIGRAM PERCENT\n",
      "SORTING NEWS BIGRAM COUNT\n",
      "SORTING NEWS TRIGRAM PERCENT\n",
      "SORTING NEWS TRIGRAM COUNT\n",
      "WRITING NEWS FILES\n",
      "WRITING NEWS unigramCounts\n",
      "WRITING NEWS unigramPercent\n",
      "WRITING NEWS bigramCounts\n",
      "WRITING NEWS bigramPercent\n",
      "WRITING NEWS trigramCounts\n",
      "WRITING NEWS trigramPercent\n",
      "2020-02-27 17:51:55\n",
      "END NEWS\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from multiprocessing import Process\n",
    "import multiprocessing\n",
    "import datetime\n",
    "\n",
    "def ngram_function(index, sentences, tokenCountDict, unigramsDict, bigramsDict, trigramsDict, stopwords):\n",
    "    sentCount = 0\n",
    "    \n",
    "    #use temp vars to avoid race conditions\n",
    "    tmpunigramsDict = []\n",
    "    tmpbigramsDict = []\n",
    "    tmptrigramsDict = []\n",
    "    tmptokenCount = 0\n",
    "    \n",
    "    for sent in sentences[index]:\n",
    "        if sentCount % 1000 == 0:\n",
    "            now = datetime.datetime.now()\n",
    "            print(\"Thread \" + str(index) + \": \" + str(sentCount) + \" of \" + str(len(sentences[index])) + \" \" + now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        for word in list(words):  \n",
    "            if word in stopwords:\n",
    "                words.remove(word)\n",
    "        tmptokenCount += len(words)\n",
    "        tmpunigramsDict += (ngrams(words, 1))\n",
    "        tmpbigramsDict += (ngrams(words, 2))\n",
    "        tmptrigramsDict += (ngrams(words, 3))\n",
    "        sentCount += 1\n",
    "    tokenCountDict[index] = tmptokenCount\n",
    "    unigramsDict[index] = tmpunigramsDict\n",
    "    bigramsDict[index] = tmpbigramsDict\n",
    "    trigramsDict[index] = tmptrigramsDict\n",
    "\n",
    "def processUnigrams(tokenCount, rawCounter, percentDict, countDict):\n",
    "    for word in rawCounter:\n",
    "        firstword = list(word)[0]\n",
    "        count = rawCounter[(firstword,)]\n",
    "        if count > 0:\n",
    "            percent = math.log(count / tokenCount)\n",
    "            percentDict.update({firstword : percent})        \n",
    "            countDict.update({firstword : count})\n",
    "        \n",
    "def processNGrams(rawCounter, reviewUGCount, percentDict, countDict):\n",
    "    for ngram in rawCounter:\n",
    "        count = rawCounter[ngram]\n",
    "        if count > 0:\n",
    "            firstword = list(ngram)[0]\n",
    "            if firstword in reviewUGCount.keys():\n",
    "                total = reviewUGCount[firstword]\n",
    "                percent = math.log(count / total)\n",
    "                percentDict.update({ngram : percent})\n",
    "                countDict.update({ngram : count})\n",
    "\n",
    "def nGramDictToCSV(path, fileName, theDict):\n",
    "    theDict = dict(theDict)\n",
    "    w = csv.writer(open(path + \"/\" + fileName + \".csv\", \"w\"))\n",
    "    for key, val in theDict.items():\n",
    "        w.writerow([key, val])\n",
    "        \n",
    "def sortDict(theDict):\n",
    "    return dict(sorted(theDict.items(), key=lambda y: y[1], reverse=True))\n",
    "\n",
    "print(\"START WINE REVIEWS\")\n",
    "reviewTokenCount = 0\n",
    "\n",
    "reviewUnigrams = Counter()\n",
    "reviewBigrams = Counter()\n",
    "reviewTrigrams = Counter()\n",
    "\n",
    "#I dont want these in my ngram results\n",
    "stopwords = [\"'\", \"s\", \"’\", \"”\", \"“\", \"t\"]\n",
    "\n",
    "print(\"CREATING NGRAM COUNTS - WINE REIVEWS\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "#This is taking too long, use all cores on the machine.\n",
    "#Match to the number of threads on the machine\n",
    "threadCount = 8\n",
    "\n",
    "#Prep for multithread\n",
    "interval = math.ceil(len(reviewSents) / threadCount)\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "reviewThreadDict = manager.dict()\n",
    "reviewUnigramsDict = manager.dict()\n",
    "reviewBigramsDict = manager.dict()\n",
    "reviewTrigramsDict = manager.dict()\n",
    "reviewTokenCountDict = manager.dict()\n",
    "\n",
    "for x in range(threadCount):\n",
    "    reviewThreadDict.update({x : reviewSents[interval*x:interval*(x+1)]})\n",
    "    reviewUnigramsDict.update({x : []})\n",
    "    reviewBigramsDict.update({x : []})\n",
    "    reviewTrigramsDict.update({x : []})\n",
    "    reviewTokenCountDict.update({x : 0})\n",
    "\n",
    "print(\"Review sentence length: \" + str(len(reviewSents)))    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    threads = list()\n",
    "    for index in range(threadCount):\n",
    "        x = Process(target=ngram_function, args=(index, reviewThreadDict, reviewTokenCountDict, reviewUnigramsDict, reviewBigramsDict, reviewTrigramsDict, stopwords))\n",
    "        threads.append(x)\n",
    "        print(\"Starting thread \" + str(index))\n",
    "        x.start()\n",
    "\n",
    "    for a in range(threadCount):\n",
    "        thread = threads[a]       \n",
    "        thread.join()        \n",
    "        reviewUnigrams += Counter(reviewUnigramsDict[a])\n",
    "        reviewBigrams += Counter(reviewBigramsDict[a])\n",
    "        reviewTrigrams += Counter(reviewTrigramsDict[a])\n",
    "        reviewTokenCount += reviewTokenCountDict[a]\n",
    "        print(\"End thread \" + str(a))\n",
    "\n",
    "print(\"All threads complete.\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "print(\"CREATING COUNTER DICTIONARY - WINE REIVEWS\")\n",
    "\n",
    "reviewNGramCounts = {\n",
    "    \"unigrams\" : Counter(reviewUnigrams),\n",
    "    \"bigrams\" : Counter(reviewBigrams),\n",
    "    \"trigrams\" : Counter(reviewTrigrams)\n",
    "}\n",
    "\n",
    "reviewUGPercent = {}\n",
    "reviewUGCount = {}\n",
    "reviewBGPercent = {}\n",
    "reviewBGCount = {}\n",
    "reviewTGPercent = {}\n",
    "reviewTGCount = {}\n",
    "\n",
    "print(\"PROCCESSING UNIGRAMS PERCENTAGE - WINE REVIEWS\")\n",
    "processUnigrams(reviewTokenCount, reviewNGramCounts[\"unigrams\"], reviewUGPercent, reviewUGCount)\n",
    "print(\"PROCCESSING BIGRAM PERCENTAGE - WINE REVIEWS\")\n",
    "processNGrams(reviewNGramCounts[\"bigrams\"], reviewUGCount, reviewBGPercent, reviewBGCount)\n",
    "print(\"PROCCESSING TRIGRAMS PERCENTAGE - WINE REVIEWS\")\n",
    "processNGrams(reviewNGramCounts[\"trigrams\"], reviewUGCount, reviewTGPercent, reviewTGCount)\n",
    "\n",
    "print(\"SORTING WINE UNIGRAM PERCENT\")\n",
    "reviewUGPercent = sortDict(reviewUGPercent)\n",
    "print(\"SORTING WINE UNIGRAM COUNT\")\n",
    "reviewUGCount = sortDict(reviewUGCount)\n",
    "print(\"SORTING WINE BIGRAM PERCENT\")\n",
    "reviewBGPercent = sortDict(reviewBGPercent)\n",
    "print(\"SORTING WINE BIGRAM COUNT\")\n",
    "reviewBGCount = sortDict(reviewBGCount)\n",
    "print(\"SORTING WINE TRIGRAM PERCENT\")\n",
    "reviewTGPercent = sortDict(reviewTGPercent)\n",
    "print(\"SORTING WINE TRIGRAM COUNT\")\n",
    "reviewTGCount = sortDict(reviewTGCount)\n",
    "\n",
    "print(\"WRITING WINE FILES\")\n",
    "print(\"WRITING WINE unigramCounts\")\n",
    "nGramDictToCSV(\"wine-reviews/\", \"unigramCounts\", reviewUGCount)\n",
    "print(\"WRITING WINE unigramPercent\")\n",
    "nGramDictToCSV(\"wine-reviews/\", \"unigramPercent\", reviewUGPercent)\n",
    "print(\"WRITING WINE bigramCounts\")\n",
    "nGramDictToCSV(\"wine-reviews/\", \"bigramCounts\", reviewBGCount)\n",
    "print(\"WRITING WINE bigramPercent\")\n",
    "nGramDictToCSV(\"wine-reviews/\", \"bigramPercent\", reviewBGPercent)\n",
    "print(\"WRITING WINE trigramCounts\")\n",
    "nGramDictToCSV(\"wine-reviews/\", \"trigramCounts\", reviewTGCount)\n",
    "print(\"WRITING WINE trigramPercent\")\n",
    "nGramDictToCSV(\"wine-reviews/\", \"trigramPercent\", reviewTGPercent)\n",
    "\n",
    "print(\"END WINE REVIEWS\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "###############################################################\n",
    "\n",
    "print(\"START NEWS\")\n",
    "articleTokenCount = 0    \n",
    "\n",
    "articleUnigrams = Counter()\n",
    "articleBigrams = Counter()\n",
    "articleTrigrams = Counter()\n",
    "\n",
    "print(\"CREATING NGRAM COUNTS - NEWS\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "#Prep for multithread\n",
    "interval = math.ceil(len(articleSents) / threadCount)\n",
    "\n",
    "articleThreadDict = manager.dict()\n",
    "articleUnigramsDict = manager.dict()\n",
    "articleBigramsDict = manager.dict()\n",
    "articleTrigramsDict = manager.dict()\n",
    "articleTokenCountDict = manager.dict()\n",
    "\n",
    "for x in range(threadCount):\n",
    "    articleThreadDict.update({x : articleSents[interval*x:interval*(x+1)]})\n",
    "    articleUnigramsDict.update({x : []})\n",
    "    articleBigramsDict.update({x : []})\n",
    "    articleTrigramsDict.update({x : []})\n",
    "    articleTokenCountDict.update({x : 0})\n",
    "\n",
    "print(\"News sentence length: \" + str(len(reviewSents)))      \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    threads = list()\n",
    "    for index in range(threadCount):\n",
    "        x = Process(target=ngram_function, args=(index, articleThreadDict, articleTokenCountDict, articleUnigramsDict, articleBigramsDict, articleTrigramsDict, stopwords))\n",
    "        threads.append(x)\n",
    "        print(\"Starting thread \" + str(index))\n",
    "        x.start()\n",
    "\n",
    "    for a in range(threadCount):\n",
    "        thread = threads[a]\n",
    "        thread.join()\n",
    "        articleUnigrams += Counter(articleUnigramsDict[a])\n",
    "        articleBigrams += Counter(articleBigramsDict[a])\n",
    "        articleTrigrams += Counter(articleTrigramsDict[a])\n",
    "        articleTokenCount += articleTokenCountDict[a]\n",
    "        print(\"End thread \" + str(a))\n",
    "        \n",
    "print(\"All threads complete\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "print(\"CREATING COUNTER DICTIONARY - NEWS\")\n",
    "articleNGramCounts = {\n",
    "    \"unigrams\" : Counter(articleUnigrams),\n",
    "    \"bigrams\" : Counter(articleBigrams),\n",
    "    \"trigrams\" : Counter(articleTrigrams)\n",
    "}\n",
    "\n",
    "articleUGPercent = {}\n",
    "articleUGCount = {}\n",
    "articleBGPercent = {}\n",
    "articleBGCount = {}\n",
    "articleTGPercent = {}\n",
    "articleTGCount = {}\n",
    "\n",
    "print(\"PROCCESSING UNIGRAMS PERCENTAGE - NEWS\")\n",
    "processUnigrams(articleTokenCount, articleNGramCounts[\"unigrams\"], articleUGPercent, articleUGCount)\n",
    "print(\"PROCCESSING BIGRAM PERCENTAGE - NEWS\")\n",
    "processNGrams(articleNGramCounts[\"bigrams\"], articleUGCount, articleBGPercent, articleBGCount)\n",
    "print(\"PROCCESSING TRIGRAM PERCENTAGE - NEWS\")\n",
    "processNGrams(articleNGramCounts[\"trigrams\"], articleUGCount, articleTGPercent, articleTGCount)\n",
    "\n",
    "print(\"SORTING NEWS UNIGRAM PERCENT\")\n",
    "articleUGPercent = sortDict(articleUGPercent)\n",
    "print(\"SORTING NEWS UNIGRAM COUNT\")\n",
    "articleUGCount = sortDict(articleUGCount)\n",
    "print(\"SORTING NEWS BIGRAM PERCENT\")\n",
    "articleBGPercent = sortDict(articleBGPercent)\n",
    "print(\"SORTING NEWS BIGRAM COUNT\")\n",
    "articleBGCount = sortDict(articleBGCount)\n",
    "print(\"SORTING NEWS TRIGRAM PERCENT\")\n",
    "articleTGPercent = sortDict(articleTGPercent)\n",
    "print(\"SORTING NEWS TRIGRAM COUNT\")\n",
    "articleTGCount = sortDict(articleTGCount)\n",
    "\n",
    "print(\"WRITING NEWS FILES\")\n",
    "print(\"WRITING NEWS unigramCounts\")\n",
    "nGramDictToCSV(\"all-the-news/\", \"unigramCounts\", articleUGCount)\n",
    "print(\"WRITING NEWS unigramPercent\")\n",
    "nGramDictToCSV(\"all-the-news/\", \"unigramPercent\", articleUGPercent)\n",
    "print(\"WRITING NEWS bigramCounts\")\n",
    "nGramDictToCSV(\"all-the-news/\", \"bigramCounts\", articleBGCount)\n",
    "print(\"WRITING NEWS bigramPercent\")\n",
    "nGramDictToCSV(\"all-the-news/\", \"bigramPercent\", articleBGPercent)\n",
    "print(\"WRITING NEWS trigramCounts\")\n",
    "nGramDictToCSV(\"all-the-news/\", \"trigramCounts\", articleTGCount)\n",
    "print(\"WRITING NEWS trigramPercent\")\n",
    "nGramDictToCSV(\"all-the-news/\", \"trigramPercent\", articleTGPercent)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "print(\"END NEWS\")\n",
    "\n",
    "print(\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Compare the statistics of the corpora.</u>\n",
    "                        \n",
    "Use the results of those calculations that you just made the poor computer painstakingly compute. What are the differences in the most common unigrams between the two language models? Are there interesting differences between the bigram models or trigram models?\n",
    "\n",
    "Be able to sort the n-grams to output the top k with the highest count or probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the,980\n",
      "a,459\n",
      "of,431\n",
      "to,425\n",
      "and,419\n",
      "in,404\n",
      "that,197\n",
      "he,186\n",
      "was,156\n",
      "for,154\n",
      "174\n",
      "3991\n"
     ]
    }
   ],
   "source": [
    "# Load ngram percents and counts from files\n",
    "# Use head 100 files for testing\n",
    "\n",
    "#Major differences in news was mainly politcal and contained words talking about the president, white hosue, us etc\n",
    "#Wine reiviews had none of this in their top ngrams\n",
    "\n",
    "def readCountPercentFiles(fp):\n",
    "    file = open(fp, \"r\")\n",
    "    tmpDict = {}\n",
    "    with file as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "        for lines in csv_reader:\n",
    "            tmpDict.update({lines[0] : lines[1]})\n",
    "    return tmpDict\n",
    "\n",
    "def writeTopN(theDict, top):\n",
    "    for x in range(top):\n",
    "        print(list(theDict.keys())[x] + \",\" + list(theDict.values())[x])\n",
    "        \n",
    "newsUGCountDict = readCountPercentFiles(\"all-the-news/unigramCounts.csv\")\n",
    "newsBGCountDict = readCountPercentFiles(\"all-the-news/bigramCounts.csv\")\n",
    "newsTGCountDict = readCountPercentFiles(\"all-the-news/trigramCounts.csv\")\n",
    "newsUGPercentDict = readCountPercentFiles(\"all-the-news/unigramPercent.csv\")\n",
    "newsBGPercentDict = readCountPercentFiles(\"all-the-news/bigramPercent.csv\")\n",
    "newsTGPercentDict = readCountPercentFiles(\"all-the-news/trigramPercent.csv\")\n",
    "\n",
    "wineUGCountDict = readCountPercentFiles(\"wine-reviews/unigramCounts.csv\")\n",
    "wineBGCountDict = readCountPercentFiles(\"wine-reviews/bigramCounts.csv\")\n",
    "wineTGCountDict = readCountPercentFiles(\"wine-reviews/trigramCounts.csv\")\n",
    "wineUGPercentDict = readCountPercentFiles(\"wine-reviews/unigramPercent.csv\")\n",
    "wineBGPercentDict = readCountPercentFiles(\"wine-reviews/bigramPercent.csv\")\n",
    "wineTGPercentDict = readCountPercentFiles(\"wine-reviews/trigramPercent.csv\")\n",
    "\n",
    "#newsUGCountDict = readCountPercentFiles(\"all-the-news/unigramCounts100.csv\")\n",
    "#newsBGCountDict = readCountPercentFiles(\"all-the-news/bigramCounts100.csv\")\n",
    "#newsTGCountDict = readCountPercentFiles(\"all-the-news/trigramCounts100.csv\")\n",
    "#newsUGPercentDict = readCountPercentFiles(\"all-the-news/unigramPercent100.csv\")\n",
    "#newsBGPercentDict = readCountPercentFiles(\"all-the-news/bigramPercent100.csv\")\n",
    "#newsTGPercentDict = readCountPercentFiles(\"all-the-news/trigramPercent100.csv\")\n",
    "\n",
    "#wineUGCountDict = readCountPercentFiles(\"wine-reviews/unigramCounts100.csv\")\n",
    "#wineBGCountDict = readCountPercentFiles(\"wine-reviews/bigramCounts100.csv\")\n",
    "#wineTGCountDict = readCountPercentFiles(\"wine-reviews/trigramCounts100.csv\")\n",
    "#wineUGPercentDict = readCountPercentFiles(\"wine-reviews/unigramPercent100.csv\")\n",
    "#wineBGPercentDict = readCountPercentFiles(\"wine-reviews/bigramPercent100.csv\")\n",
    "#wineTGPercentDict = readCountPercentFiles(\"wine-reviews/trigramPercent100.csv\")\n",
    "\n",
    "#newsUGCountDict = readCountPercentFiles(\"all-the-news/unigramCountsFull.csv\")\n",
    "#newsBGCountDict = readCountPercentFiles(\"all-the-news/bigramCountsFull.csv\")\n",
    "#newsTGCountDict = readCountPercentFiles(\"all-the-news/trigramCountsFull.csv\")\n",
    "#newsUGPercentDict = readCountPercentFiles(\"all-the-news/unigramPercentFull.csv\")\n",
    "#newsBGPercentDict = readCountPercentFiles(\"all-the-news/bigramPercentFull.csv\")\n",
    "#newsTGPercentDict = readCountPercentFiles(\"all-the-news/trigramPercentFull.csv\")\n",
    "\n",
    "#wineUGCountDict = readCountPercentFiles(\"wine-reviews/unigramCountsFull.csv\")\n",
    "#wineBGCountDict = readCountPercentFiles(\"wine-reviews/bigramCountsFull.csv\")\n",
    "#wineTGCountDict = readCountPercentFiles(\"wine-reviews/trigramCountsFull.csv\")\n",
    "#wineUGPercentDict = readCountPercentFiles(\"wine-reviews/unigramPercentFull.csv\")\n",
    "#wineBGPercentDict = readCountPercentFiles(\"wine-reviews/bigramPercentFull.csv\")\n",
    "#wineTGPercentDict = readCountPercentFiles(\"wine-reviews/trigramPercentFull.csv\")\n",
    "\n",
    "#print top N from dict\n",
    "#csv's are already sorted\n",
    "writeTopN(newsUGCountDict, 10)\n",
    "\n",
    "print(len(wineUGCountDict))\n",
    "print(len(newsUGCountDict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news unigrams\n",
      "the,980\n",
      "a,459\n",
      "of,431\n",
      "to,425\n",
      "and,419\n",
      "in,404\n",
      "that,197\n",
      "he,186\n",
      "was,156\n",
      "for,154\n",
      "on,150\n",
      "his,148\n",
      "said,119\n",
      "mr,108\n",
      "they,106\n",
      "news bigrams\n",
      "('in', 'the'),127\n",
      "('of', 'the'),99\n",
      "('and', 'the'),42\n",
      "('in', 'a'),41\n",
      "('on', 'the'),39\n",
      "('to', 'the'),37\n",
      "('he', 'said'),30\n",
      "('mr', 'wong'),30\n",
      "('ms', 'kerr'),27\n",
      "('at', 'the'),26\n",
      "('he', 'was'),26\n",
      "('the', '40th'),25\n",
      "('40th', 'precinct'),24\n",
      "('mr', 'leahy'),23\n",
      "('for', 'the'),22\n",
      "news trigrams\n",
      "('the', '40th', 'precinct'),22\n",
      "('in', 'the', 'city'),13\n",
      "('the', 'united', 'states'),12\n",
      "('in', 'the', '40th'),10\n",
      "('in', 'the', 'precinct'),9\n",
      "('in', 'the', 'bronx'),9\n",
      "('mr', 'wong', 'was'),8\n",
      "('more', 'than', 'a'),7\n",
      "('one', 'of', 'the'),6\n",
      "('of', 'the', 'precinct'),6\n",
      "('the', 'biggest', 'loser'),6\n",
      "('calories', 'a', 'day'),6\n",
      "('the', 'police', 'department'),5\n",
      "('the', 'betances', 'houses'),5\n",
      "('back', 'to', 'the'),5\n",
      "wine unigrams\n",
      "and,16\n",
      "the,10\n",
      "with,10\n",
      "a,9\n",
      "acidity,6\n",
      "this,6\n",
      "of,6\n",
      "is,5\n",
      "wine,5\n",
      "its,5\n",
      "aromas,4\n",
      "flavors,4\n",
      "in,4\n",
      "dried,3\n",
      "palate,3\n",
      "wine bigrams\n",
      "('the', 'palate'),3\n",
      "('and', 'dried'),2\n",
      "('this', 'is'),2\n",
      "('with', 'acidity'),2\n",
      "('the', 'flavors'),2\n",
      "('flavors', 'of'),2\n",
      "('to', 'a'),2\n",
      "('in', 'this'),2\n",
      "('balanced', 'with'),2\n",
      "('acidity', 'and'),2\n",
      "('aromas', 'include'),1\n",
      "('include', 'tropical'),1\n",
      "('tropical', 'fruit'),1\n",
      "('fruit', 'broom'),1\n",
      "('broom', 'brimstone'),1\n",
      "wine trigrams\n",
      "('aromas', 'include', 'tropical'),1\n",
      "('include', 'tropical', 'fruit'),1\n",
      "('tropical', 'fruit', 'broom'),1\n",
      "('fruit', 'broom', 'brimstone'),1\n",
      "('broom', 'brimstone', 'and'),1\n",
      "('brimstone', 'and', 'dried'),1\n",
      "('and', 'dried', 'herb'),1\n",
      "('the', 'palate', 'isnt'),1\n",
      "('palate', 'isnt', 'overly'),1\n",
      "('isnt', 'overly', 'expressive'),1\n",
      "('overly', 'expressive', 'offering'),1\n",
      "('expressive', 'offering', 'unripened'),1\n",
      "('offering', 'unripened', 'apple'),1\n",
      "('unripened', 'apple', 'citrus'),1\n",
      "('apple', 'citrus', 'and'),1\n"
     ]
    }
   ],
   "source": [
    "print(\"news unigrams\")\n",
    "writeTopN(newsUGCountDict, 15)\n",
    "print(\"news bigrams\")\n",
    "writeTopN(newsBGCountDict, 15)\n",
    "print(\"news trigrams\")\n",
    "writeTopN(newsTGCountDict, 15)\n",
    "print(\"wine unigrams\")\n",
    "writeTopN(wineUGCountDict, 15)\n",
    "print(\"wine bigrams\")\n",
    "writeTopN(wineBGCountDict, 15)\n",
    "print(\"wine trigrams\")\n",
    "writeTopN(wineTGCountDict, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Generate random sentences from the N-grams models for both datasets.</u>\n",
    "                        \n",
    "We briefly talked about this idea in class. It's also introduced at a high-level in J&M 4.3. How can a random number in the range [0,1] probabilistically generate a word using your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine\n",
      "a pleasantly with aromas restrained nonetheless much if with plum a hearty elegant with regular for country expressive wine crisp \n",
      "this is fresh with juicy red berry fruits and savory \n",
      "a typical navarran whiff of preserved peach in this is fairly full bodied with tomatoey was all stainlesssteel fermented offers spice \n",
      "news\n",
      "people the father hair chiefs states former hungry in their 40th monarch is epithet parts on of problem 2017 \n",
      "potentially decision that will of the contract between 152 and brains of retribution \n",
      "police say those crimes the pounds come out of leadership changes in the successful ground was different \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from math import exp\n",
    "from random import randint\n",
    "\n",
    "def getDictCount(theDict):\n",
    "    count = 0\n",
    "    for w in theDict:\n",
    "        count += int(theDict[w])\n",
    "    return count\n",
    "\n",
    "def weighted_random_by_dct(dct, count):\n",
    "    rand_val = random.random()\n",
    "    total = 0\n",
    "    for k, v in dct.items():\n",
    "        total += float(v) / count\n",
    "        if rand_val <= total:\n",
    "            return k\n",
    "\n",
    "def findGram(firstword, theDict):\n",
    "    fwordlist = {}\n",
    "    fwordcount = 0\n",
    "    for w in theDict:\n",
    "        theKey = eval(w)[0]\n",
    "        if(theKey == firstword):\n",
    "            fwordlist.update({w : theDict[w]})\n",
    "            fwordcount += int(theDict[w])\n",
    "    return weighted_random_by_dct(fwordlist, fwordcount)\n",
    "    return randint(0, 1)\n",
    "\n",
    "def makeTriSentence(setChoice):\n",
    "    uniDict = newsUGCountDict\n",
    "    triDict = newsTGCountDict\n",
    "   \n",
    "    if setChoice == \"wine\":\n",
    "        uniDict = wineUGCountDict\n",
    "        triDict = wineTGCountDict\n",
    "    \n",
    "    uniCount = getDictCount(uniDict)\n",
    "    triCount = getDictCount(triDict)\n",
    "    \n",
    "    firstword = weighted_random_by_dct(uniDict, uniCount)\n",
    "    \n",
    "    numtri = randint(6, 10)\n",
    "    \n",
    "    sentGrams = firstword + \" \"\n",
    "    while numtri >= 0:\n",
    "        theGram = findGram(firstword, triDict)\n",
    "        if theGram is not None:\n",
    "            tupp = eval(theGram)\n",
    "            tupp = list(tupp)\n",
    "            sentGrams += tupp[1] + \" \" + tupp[2]  + \" \"\n",
    "            firstword = eval(theGram)[2]\n",
    "            numtri = numtri - 1\n",
    "        else:\n",
    "            firstword = weighted_random_by_dct(uniDict, uniCount)\n",
    "           \n",
    "    return sentGrams\n",
    "\n",
    "def makeBiSentence(setChoice):\n",
    "    uniDict = newsUGCountDict\n",
    "    biDict = newsBGCountDict\n",
    "   \n",
    "    if setChoice == \"wine\":\n",
    "        uniDict = wineUGCountDict\n",
    "        biDict = wineBGCountDict\n",
    "    \n",
    "    uniCount = getDictCount(uniDict)\n",
    "    biCount = getDictCount(biDict)\n",
    "    \n",
    "    firstword = weighted_random_by_dct(uniDict, uniCount)\n",
    "    \n",
    "    numbi = randint(8, 15)\n",
    "    \n",
    "    sentGrams = firstword + \" \"\n",
    "    while numbi >= 0:\n",
    "        theGram = findGram(firstword, biDict)\n",
    "        if theGram is not None:\n",
    "            tupp = eval(theGram)\n",
    "            tupp = list(tupp)\n",
    "            sentGrams += tupp[1] + \" \"\n",
    "            firstword = eval(theGram)[1]\n",
    "            numbi = numbi - 1\n",
    "        else:\n",
    "            firstword = weighted_random_by_dct(uniDict, uniCount)\n",
    "           \n",
    "    return sentGrams\n",
    "\n",
    "def makeUniSentence(setChoice):\n",
    "    uniDict = newsUGCountDict\n",
    "   \n",
    "    if setChoice == \"wine\":\n",
    "        uniDict = wineUGCountDict\n",
    "    \n",
    "    uniCount = getDictCount(uniDict)\n",
    "    \n",
    "    firstword = weighted_random_by_dct(uniDict, uniCount)\n",
    "    \n",
    "    numuni = randint(10, 20)\n",
    "    \n",
    "    sentGrams = firstword + \" \"\n",
    "    while numuni >= 0:\n",
    "        firstword = weighted_random_by_dct(uniDict, uniCount)\n",
    "        sentGrams += firstword + \" \"\n",
    "        numuni = numuni - 1\n",
    "           \n",
    "    return sentGrams\n",
    "    \n",
    "print(\"wine\")\n",
    "print(makeUniSentence(\"wine\"))\n",
    "print(makeBiSentence(\"wine\"))\n",
    "print(makeTriSentence(\"wine\"))\n",
    "\n",
    "print(\"news\")\n",
    "print(makeUniSentence(\"news\"))\n",
    "print(makeBiSentence(\"news\"))\n",
    "print(makeTriSentence(\"news\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a technical report (in this Jupyter Notebook, with good Markdown formatting) that documents your findings, \"lessons learned\", any areas of where you ran into difficult, and also any other interesting details. Include in your report the following details:\n",
    "\n",
    "1. Names of the datasets used.\n",
    "1. Does your model use all of the data in the .csv file or only a subset of it (i.e. first 1,000 rows)?\n",
    "1. What is the vocabulary and size of each dataset?\n",
    "1. How did you handle the merging of separate rows in a .csv file? How did you handle sentence segmentation with sentence boundary markers? Also report on any other decisions made in step #3.\n",
    "1. How long did it take your program to build these models? Do you have any statistics on memory/RAM usage?\n",
    "1. Output the top 15 unigrams, bigrams, trigrams for each model. Are there any interesting differences?\n",
    "1. Output 3 different randomly generated sentences for each unigram, bigram, trigram model. How did you know where the randomly generated sentence ended?\n",
    "\n",
    "Also submit this python notebook `.ipynb` to D2L.\n",
    "\n",
    "<u>Perform sentence segmentation and word tokenization.</u>\n",
    "\n",
    "Utilize the nltk module to perform sentence segmentation and word tokenization. But at this point, there are a few decisions that need to be made:\n",
    "\n",
    "* How we should handle the .csv rows in the previous step? If we ignore row makers, and \"lump everything together\", how will that effect our language model?\n",
    "* Do we want to remove punctuation? What is the effect of keeping punctuation in the model?\n",
    "* Do we want to add sentence boundary markers, such as <samp>&lt;S&gt;</samp> and <samp>&lt;/S&gt;</samp>?</li>\n",
    "* Should two the words <samp>The</samp> and <samp>the</samp> be treated as the same? What are the effects of doing, or not doing, this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Report\n",
    "\n",
    "### Data Sets\n",
    "\n",
    "### <a href=\"https://www.kaggle.com/snapcrack/all-the-news\">All the news</a>\n",
    "News articles from the New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, National Review, New York Post, the Guardian, NPR, Reuters, Vox, and the Washington Post.\n",
    "Only articles1.csv from the data set was used for anylisis.\n",
    "<u>article1.csv</u>\n",
    "\n",
    "* 50,000 news articles\n",
    "* 194.11 MB\n",
    "\n",
    "### <a href=\"https://www.kaggle.com/zynicide/wine-reviews\">Wine Reviewss</a>\n",
    "130,000 Wine reviews scraped from <a href=\"http://www.winemag.com/?s=&drink_type=wine\"> WineEnthusiast</a> during June 2017.\n",
    "Only winemag-data-130k-v2.csv from the data set was used for anaylisis.\n",
    "<u>winemag-data-130k-v2.csv</u>\n",
    "* 130,000 wire reviews\n",
    "* 50.46 MB\n",
    "\n",
    "The n-gram models were created using the entire file. 2 CSV files were created for each n-gram model, one for counts and one for percentages, totaling 6 files.\n",
    "\n",
    "### News n-gram files\n",
    "\n",
    "<u>unigramCountsFull.csv</u> - 2.2 MB -177449 lines<br>\n",
    "<u>unigramPercentFull.csv</u> - 5.3 MB - 177449 lines<br>\n",
    "<u>bigramCountsFull.csv</u> - 124.3 MB -4719838 lines<br>\n",
    "<u>bigramPercentFull.csv</u> - 203.8 MB - 4719838 lines<br>\n",
    "<u>trigramCountsFull.csv</u> - 494.5 MB - 14689607 lines<br>\n",
    "<u>trigramPercentFull.csv</u> - 756.7 MB - 14689607 lines<br>\n",
    "\n",
    "### Wine n-gram files\n",
    "\n",
    "<u>unigramCountsFull.csv</u> - 584.3 kB -44771 lines<br>\n",
    "<u>unigramPercentFull.csv</u> - 1.4 MB - 44771 lines<br>\n",
    "<u>bigramCountsFull.csv</u> - 14.1 MB -535868 lines<br>\n",
    "<u>bigramPercentFull.csv</u> - 22.9 MB - 535868 lines<br>\n",
    "<u>trigramCountsFull.csv</u> - 51.8 MB - 1527733 lines<br>\n",
    "<u>trigramPercentFull.csv</u> - 77.7 MB - 1527733 lines<br>\n",
    "\n",
    "Total vocabulary can be represented by the line counts of each data set's unigram count file\n",
    "\n",
    "* News vocabulary count - 177449\n",
    "* Wine vocabulary count - 44771\n",
    "\n",
    "### Sentence Segmentation and Word Tokenization\n",
    "After column containing the articles/reviews from the data set csv files, the entries were added together into a single string with a space (\" \") seperating each entry to prevent sentences from running together.  Sentence segmation was then performed on this string using NLTK's `sent_tokenize()` function.  Each sentence was then broken into its correspoding words using NLTK's `word_tokenize()` function, after the word list for a sentence was created it was used the count the 3 different n-grams contained within it.  Counting the n-grams this way, sentence by sentence, ensured that n-grams did not overflow between sentences.  Special characters and punctuation were removed from the words and all words were made lower case before counting n-grams.\n",
    "\n",
    "### Program Performance\n",
    "The code contained within this notebook will be able to parse the entire files from both data sets only if there is sufficent RAM on the machine (16+ GB). Initially I was having problems with the amount if time it was taking to process the News data set.  This was taking over 8 hours.  I rewrote the code that counted the n-grams so the data could be processed on multiple cpus at the same time to cut the processing time.  I eventually found the problem in my code that was causing the excessive processing times. The issue was that two counter objects were being added together after each sentence was processed.  As the sum Counter object grew larger, this addtion took more time as well.  I changed this to only update a counter object every 150000 sentences.  This ran much quicker, but at the cost of more RAM utilziation.  Each file can be processed in less than 5 minutes.\n",
    "\n",
    "### News Top N-Grams\n",
    "\n",
    "#### Unigrams\n",
    "the,1873788<br>\n",
    "to,891377<br>\n",
    "of,810531<br>\n",
    "a,767762<br>\n",
    "and,757945<br>\n",
    "in,649374<br>\n",
    "that,452533<br>\n",
    "for,300547<br>\n",
    "is,295052<br>\n",
    "on,292685<br>\n",
    "he,256274<br>\n",
    "it,252612<br>\n",
    "was,226418<br>\n",
    "with,211355<br>\n",
    "said,208127<br>\n",
    "\n",
    "#### Bigrams\n",
    "('of', 'the'),192662<br>\n",
    "('in', 'the'),155857<br>\n",
    "('to', 'the'),88735<br>\n",
    "('on', 'the'),66703<br>\n",
    "('for', 'the'),53846<br>\n",
    "('at', 'the'),51722<br>\n",
    "('in', 'a'),51372<br>\n",
    "('and', 'the'),49236<br>\n",
    "('to', 'be'),48117<br>\n",
    "('that', 'the'),47247<br>\n",
    "('with', 'the'),38812<br>\n",
    "('from', 'the'),35589<br>\n",
    "('of', 'a'),33050<br>\n",
    "('as', 'a'),33041<br>\n",
    "('he', 'said'),32528<br>\n",
    "\n",
    "#### Trigrams\n",
    "('the', 'united', 'states'),21374<br>\n",
    "('one', 'of', 'the'),14553<br>\n",
    "('the', 'white', 'house'),9812<br>\n",
    "('according', 'to', 'the'),9302<br>\n",
    "('a', 'lot', 'of'),8498<br>\n",
    "('the', 'new', 'york'),6098<br>\n",
    "('as', 'well', 'as'),6017<br>\n",
    "('said', 'in', 'a'),5930<br>\n",
    "('in', 'a', 'statement'),5795<br>\n",
    "('in', 'the', 'united'),5598<br>\n",
    "('some', 'of', 'the'),5331<br>\n",
    "('new', 'york', 'times'),5061<br>\n",
    "('part', 'of', 'the'),4985<br>\n",
    "('to', 'be', 'a'),4899<br>\n",
    "('going', 'to', 'be'),4749<br>\n",
    "\n",
    "\n",
    "### Wine Top N-Grams\n",
    "\n",
    "#### Unigrams\n",
    "and,404954<br>\n",
    "the,258453<br>\n",
    "a,215622<br>\n",
    "of,184159<br>\n",
    "with,152655<br>\n",
    "is,111531<br>\n",
    "this,109710<br>\n",
    "wine,87624<br>\n",
    "flavors,77830<br>\n",
    "in,74619<br>\n",
    "to,64069<br>\n",
    "it,63720<br>\n",
    "its,60771<br>\n",
    "fruit,56501<br>\n",
    "but,48420<br>\n",
    "\n",
    "#### Bigrams\n",
    "('on', 'the'),34841<br>\n",
    "('this', 'is'),27562<br>\n",
    "('is', 'a'),24238<br>\n",
    "('in', 'the'),22983<br>\n",
    "('the', 'palate'),21646<br>\n",
    "('and', 'a'),20614<br>\n",
    "('the', 'wine'),19738<br>\n",
    "('the', 'finish'),19696<br>\n",
    "('flavors', 'of'),19612<br>\n",
    "('aromas', 'of'),14910<br>\n",
    "('of', 'the'),14066<br>\n",
    "('this', 'wine'),14013<br>\n",
    "('the', 'nose'),12380<br>\n",
    "('wine', 'is'),12180<br>\n",
    "\n",
    "#### Trigrams\n",
    "('this', 'is', 'a'),14062<br>\n",
    "('on', 'the', 'finish'),10524<br>\n",
    "('in', 'the', 'mouth'),8994<br>\n",
    "('on', 'the', 'palate'),7633<br>\n",
    "('the', 'wine', 'is'),7256<br>\n",
    "('on', 'the', 'nose'),6931<br>\n",
    "('the', 'palate', 'is'),6179<br>\n",
    "('a', 'touch', 'of'),5641<br>\n",
    "('a', 'hint', 'of'),4099<br>\n",
    "('the', 'finish', 'is'),3437<br>\n",
    "('the', 'mouth', 'with'),2841<br>\n",
    "('this', 'wine', 'is'),2830<br>\n",
    "('over', 'the', 'next'),2670<br>\n",
    "('as', 'well', 'as'),2544<br>\n",
    "('ready', 'to', 'drink'),2487<br>\n",
    "\n",
    "\n",
    "### Sentence Generation\n",
    "The first word in the sentence was selecetd by using a weighted algorithm to randomly select a word from the unigrams model.  This word was then used to create a list of ngrams whos first word is the first word we selected earlier.  We then select our ngram for the sentence using the same alogrithm from the first word.  The last word of the ngram is then used as the first word for the next ngram in the sentence and the process repeats until we have reached the desired length of ngrams. The number of ngrams in the sentence is determined by a random number generated between 8 and 20.\n",
    "\n",
    "### News Model Generated Sentences\n",
    "\n",
    "Unigram - antiquated block his the like with administration central sitting unsustainable how manhattan studies been the of now to from islam him his <br>\n",
    "Bigram - barack obama the interview when he added she believes in communicating with sweden but the slave labor<br>\n",
    "Trigram - them to the petitions committee on april 27 but not overwhelming during the kingdom said john rubey chief executive<br>\n",
    "\n",
    "### Wine Review Model Generated Sentences\n",
    "\n",
    "Unigram - mineral this but and also mourvèdre but ragù berries greenness in rich fruit fruit reflects light a with <br>\n",
    "Bigram - this case for the nose its maturation a whiff of tang <br>\n",
    "Trigram - to balance the most beautiful example of washington rieslings this is a good everyday quaff at a decent introduction <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
