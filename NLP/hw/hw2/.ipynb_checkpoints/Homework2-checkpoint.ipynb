{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: N-gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Out: Thursday, February 20\n",
    "## Due Date: Thursday, March 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This programming assignment is more open-ended than the previous ones. It is centered on the N-gram language models and tasks you to:\n",
    "\n",
    "* download and process a large text dataset in python using the <code>csv</code> library\n",
    "* perform sentence and word tokenization\n",
    "* calculate N-gram counts and probabilities\n",
    "* compare the characteristics of the N-grams across different models\n",
    "* generate random sentences using the models\n",
    "\n",
    "<u>You may work in teams of two or three (2-tuples or 3-tuples?) for this assignment.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Download two large text datasets from Kaggle.</u>\n",
    "\n",
    "The <a href=\"http://kaggle.com\">Kaggle competition hosting site</a> offers a number of free datasets that contain interesting text fields. For this assignment, we will use the \"Wine Reviews\" and \"All the News\" datasets. They can be accessed by selecting the \"Datasets\" header and then searching for these specific datasets. Then, choose \"Data\" from the sub-header, preview some of the csv data and notice how at least one of the columns in the dataset will contain sufficient text. I chose to direct you to these two datasets because the textual content seemed interesting and would have different language characteristics, and both were large csv files that could generate significant n-gram counts, but not be too large of a file.\n",
    "\n",
    "<em>(You can use other datasets if you wish. Others that looked interesting on Kaggle include the \"Yelp Dataset\" (but its over 3GB !!!), \"SMS Spam Collection Dataset\", \"Russian Troll Tweets\", and \"A Million News Headlines\".)</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloaded wine-reviews and all-the-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Process the downloaded <code>csv</code> files in python.</u>\n",
    "\n",
    "There's a nice csv library already included in python for accessing values in that are stored in a comma separated values (csv) format. Read the <a href=\"https://docs.python.org/3/library/csv.html\">csv library documentation</a>.\n",
    "What is the delimiter in your csv files? Open each of the two .csv files that you downloaded using this library and be able to read in the data. Note that we really only care about the text column in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'all-the-news/head.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-957647040c00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#news = open(\"all-the-news/head50.csv\", \"r\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#wine = open(\"wine-reviews/head50.csv\", \"r\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mnews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all-the-news/head.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mwine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wine-reviews/head.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'all-the-news/head.csv'"
     ]
    }
   ],
   "source": [
    "# Use the head.csv file in each folder for testing\n",
    "# Both csv files are comma deliminated\n",
    "\n",
    "#Field limit\n",
    "import sys\n",
    "import csv\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "\n",
    "#news = open(\"all-the-news/articles1.csv\", \"r\")\n",
    "#wine = open(\"wine-reviews/winemag-data_first150k.csv\", \"r\")\n",
    "#news = open(\"all-the-news/head1000.csv\", \"r\")\n",
    "#wine = open(\"wine-reviews/head1000.csv\", \"r\")\n",
    "#news = open(\"all-the-news/head500.csv\", \"r\")\n",
    "#wine = open(\"wine-reviews/head500.csv\", \"r\")\n",
    "#news = open(\"all-the-news/head50.csv\", \"r\")\n",
    "#wine = open(\"wine-reviews/head50.csv\", \"r\")\n",
    "news = open(\"all-the-news/head.csv\", \"r\")\n",
    "wine = open(\"wine-reviews/head.csv\", \"r\")\n",
    "\n",
    "reviews = []\n",
    "articles = []\n",
    "with news as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=\",\")\n",
    "    for lines in csv_reader:\n",
    "        articles.append(lines[\"content\"])\n",
    "with wine as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=\",\")\n",
    "    for lines in csv_reader:\n",
    "        reviews.append(lines[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(reviews)\n",
    "#print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Perform sentence segmentation and word tokenization.</u>\n",
    "\n",
    "Utilize the nltk module to perform sentence segmentation and word tokenization. But at this point, there are a few decisions that need to be made:\n",
    "\n",
    "* How we should handle the .csv rows in the previous step? If we ignore row makers, and \"lump everything together\", how will that effect our language model?\n",
    "* Do we want to remove punctuation? What is the effect of keeping punctuation in the model?\n",
    "* Do we want to add sentence boundary markers, such as <samp>&lt;S&gt;</samp> and <samp>&lt;/S&gt;</samp>?</li>\n",
    "* Should two the words <samp>The</samp> and <samp>the</samp> be treated as the same? What are the effects of doing, or not doing, this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READ REVIEWS\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7a14f35225f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0marticlesraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"READ REVIEWS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m#review = review.translate(str.maketrans('', '', string.punctuation))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reviews' is not defined"
     ]
    }
   ],
   "source": [
    "# Rows will be lumped into a single string.  A single \" \" will be added to the end of each row to ensure sentences\n",
    "# are not being combined (Ex. \"lastword. firstword\" instead of \"lastword.firstword\")\n",
    "# Punctuation will NOT be kept when counting n-grams\n",
    "# The and the will be treated as the same word, this will decrease total n-grams\n",
    "# N-Gram words will be counted within sentence boundries  (a trigram/bigram will not overlap into another sentence)\n",
    "import string\n",
    "import datetime\n",
    "\n",
    "reviewsraw = \"\"\n",
    "articlesraw = \"\"\n",
    "print(\"READ REVIEWS\")\n",
    "for review in reviews:\n",
    "    #review = review.translate(str.maketrans('', '', string.punctuation))\n",
    "    review = review.lower()\n",
    "    reviewsraw += review + \" \"\n",
    "print(\"READ REVIEWS COMPLETE\")\n",
    "print(\"READ NEWS\")\n",
    "for article in articles:\n",
    "    #article = article.translate(str.maketrans('', '', string.punctuation))\n",
    "    article = article.lower()\n",
    "    articlesraw += article + \" \"\n",
    "print(\"READ NEWS COMPLETE\")\n",
    "    \n",
    "#reviewTokens = nltk.word_tokenize(reviewsraw)\n",
    "print(\"SENT TOKE REVIEWS\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "reviewSents = nltk.sent_tokenize(reviewsraw)\n",
    "print(\"SENT TOKE REVIEWS COMPLETE\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(\"SENT TOKE NEWS\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "#articleTokens = nltk.word_tokenize(articlesraw)\n",
    "articleSents = nltk.sent_tokenize(articlesraw)\n",
    "print(\"SENT TOKE NEWS COMPLETE\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(reviewTokens)\n",
    "#print(reviewSents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Calculate N-gram counts and compute probabilities.</u>\n",
    "\n",
    "Use a python dictionary (or any suitable data structure) to first compute unigram counts. Then try bigram counts. Finally, trigram counts.\n",
    "\n",
    "How much memory are you using? How fast, or slow, is the code -- how long is this step taking? If it is taking too long, try only using a fraction of your corpus: instead of loading the entire .csv file, try only reading the first 1000 rows of data.\n",
    "\n",
    "Using those counts, compute the probabilities for the unigrams, bigrams, and trigrams, and store those in a new python dictionary (or some other data structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START WINE REVIEWS\n",
      "CREATING NGRAM COUNTS - WINE REIVEWS\n",
      "2020-02-24 15:00:59\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reviewSents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-25bb498d25ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m#Prep for multithread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0minterval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviewSents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mthreadCount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reviewSents' is not defined"
     ]
    }
   ],
   "source": [
    "# Due to restricting the ngrams to sentence boundries, updating the ngram count dicts was taking a very\n",
    "# long time.  Parsing the news articles took over 10 hours with the sentences split between 8 threads\n",
    "# as the size the ngram count dictionary increased so did the time it took to update it with each consecutive sentence.\n",
    "# I ended up moving this to the Bridges super computer.  Running the process on 28 cores, each thread had to handle\n",
    "# only 50000 sentences (as oppsoed ot the 180000 sentences per core on my 8 core desktop). \n",
    "# this ran in about 40 minutes.\n",
    "# I have only included head stubs of the files for my submision. The full files totaled over 1.5 GB.\n",
    "# If you would like to see the full files please let me know.\n",
    "\n",
    "import math\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from multiprocessing import Process\n",
    "import multiprocessing\n",
    "import datetime\n",
    "\n",
    "def ngram_function(index, sentences, tokenCountDict, unigramsDict, bigramsDict, trigramsDict, stopwords):\n",
    "    sentCount = 0\n",
    "    \n",
    "    #use temp vars to avoid race conditions\n",
    "    tmpunigramsDict = Counter()\n",
    "    tmpbigramsDict = Counter()\n",
    "    tmptrigramsDict = Counter()\n",
    "    tmptokenCount = 0\n",
    "    \n",
    "    for sent in sentences[index]:\n",
    "        if sentCount % 1000 == 0:\n",
    "            now = datetime.datetime.now()\n",
    "            print(\"Thread \" + str(index) + \": \" + str(sentCount) + \" of \" + str(len(sentences[index])) + \" \" + now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        for word in list(words):  \n",
    "            if word in stopwords:\n",
    "                words.remove(word)\n",
    "        tmptokenCount += len(words)\n",
    "        tmpunigramsDict += Counter(ngrams(words, 1))\n",
    "        tmpbigramsDict += Counter(ngrams(words, 2))\n",
    "        tmptrigramsDict += Counter(ngrams(words, 3))\n",
    "        sentCount += 1\n",
    "    tokenCountDict[index] = tmptokenCount\n",
    "    unigramsDict[index] = tmpunigramsDict\n",
    "    bigramsDict[index] = tmpbigramsDict\n",
    "    trigramsDict[index] = tmptrigramsDict\n",
    "\n",
    "def processUnigrams(tokenCount, rawCounter, percentDict, countDict):\n",
    "    for word in rawCounter:\n",
    "        firstword = list(word)[0]\n",
    "        count = rawCounter[(firstword,)]\n",
    "        if count > 0:\n",
    "            percent = math.log(count / tokenCount)\n",
    "            percentDict.update({firstword : percent})        \n",
    "            countDict.update({firstword : count})\n",
    "        \n",
    "def processNGrams(rawCounter, reviewUGCount, percentDict, countDict):\n",
    "    for ngram in rawCounter:\n",
    "        count = rawCounter[ngram]\n",
    "        if count > 0:\n",
    "            firstword = list(ngram)[0]\n",
    "            if firstword in reviewUGCount.keys():\n",
    "                total = reviewUGCount[firstword]\n",
    "                percent = math.log(count / total)\n",
    "                percentDict.update({ngram : percent})\n",
    "                countDict.update({ngram : count})\n",
    "\n",
    "def nGramDictToCSV(path, fileName, theDict):\n",
    "    theDict = dict(theDict)\n",
    "    w = csv.writer(open(path + \"/\" + fileName + \".csv\", \"w\"))\n",
    "    for key, val in theDict.items():\n",
    "        w.writerow([key, val])\n",
    "        \n",
    "def sortDict(theDict):\n",
    "    return dict(sorted(theDict.items(), key=lambda y: y[1], reverse=True))\n",
    "\n",
    "print(\"START WINE REVIEWS\")\n",
    "reviewTokenCount = 0\n",
    "\n",
    "reviewUnigrams = Counter()\n",
    "reviewBigrams = Counter()\n",
    "reviewTrigrams = Counter()\n",
    "\n",
    "#I dont want these in my ngram results\n",
    "stopwords = [\"'\", \"s\", \"’\", \"”\", \"“\", \"t\"]\n",
    "\n",
    "print(\"CREATING NGRAM COUNTS - WINE REIVEWS\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "#This is taking too long, use all cores on the machine.\n",
    "#Match to the number of threads on the machine\n",
    "threadCount = 8\n",
    "\n",
    "#Prep for multithread\n",
    "interval = math.ceil(len(reviewSents) / threadCount)\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "reviewThreadDict = manager.dict()\n",
    "reviewUnigramsDict = manager.dict()\n",
    "reviewBigramsDict = manager.dict()\n",
    "reviewTrigramsDict = manager.dict()\n",
    "reviewTokenCountDict = manager.dict()\n",
    "\n",
    "for x in range(threadCount):\n",
    "    reviewThreadDict.update({x : reviewSents[interval*x:interval*(x+1)]})\n",
    "    reviewUnigramsDict.update({x : {}})\n",
    "    reviewBigramsDict.update({x : {}})\n",
    "    reviewTrigramsDict.update({x : {}})\n",
    "    reviewTokenCountDict.update({x : 0})\n",
    "\n",
    "print(\"Review sentence length: \" + str(len(reviewSents)))    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    threads = list()\n",
    "    for index in range(threadCount):\n",
    "        x = Process(target=ngram_function, args=(index, reviewThreadDict, reviewTokenCountDict, reviewUnigramsDict, reviewBigramsDict, reviewTrigramsDict, stopwords))\n",
    "        threads.append(x)\n",
    "        print(\"Starting thread \" + str(index))\n",
    "        x.start()\n",
    "\n",
    "    for a in range(threadCount):\n",
    "        thread = threads[a]       \n",
    "        thread.join()        \n",
    "        reviewUnigrams += reviewUnigramsDict[a]\n",
    "        reviewBigrams += reviewBigramsDict[a]\n",
    "        reviewTrigrams += reviewTrigramsDict[a]\n",
    "        reviewTokenCount += reviewTokenCountDict[a]\n",
    "        print(\"End thread \" + str(a))\n",
    "\n",
    "print(\"All threads complete.\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "print(\"CREATING COUNTER DICTIONARY - WINE REIVEWS\")\n",
    "\n",
    "reviewNGramCounts = {\n",
    "    \"unigrams\" : Counter(reviewUnigrams),\n",
    "    \"bigrams\" : Counter(reviewBigrams),\n",
    "    \"trigrams\" : Counter(reviewTrigrams)\n",
    "}\n",
    "\n",
    "reviewUGPercent = {}\n",
    "reviewUGCount = {}\n",
    "reviewBGPercent = {}\n",
    "reviewBGCount = {}\n",
    "reviewTGPercent = {}\n",
    "reviewTGCount = {}\n",
    "\n",
    "print(\"PROCCESSING UNIGRAMS PERCENTAGE - WINE REVIEWS\")\n",
    "processUnigrams(reviewTokenCount, reviewNGramCounts[\"unigrams\"], reviewUGPercent, reviewUGCount)\n",
    "print(\"PROCCESSING BIGRAM PERCENTAGE - WINE REVIEWS\")\n",
    "processNGrams(reviewNGramCounts[\"bigrams\"], reviewUGCount, reviewBGPercent, reviewBGCount)\n",
    "print(\"PROCCESSING TRIGRAMS PERCENTAGE - WINE REVIEWS\")\n",
    "processNGrams(reviewNGramCounts[\"trigrams\"], reviewUGCount, reviewTGPercent, reviewTGCount)\n",
    "\n",
    "print(\"SORTING WINE UNIGRAM PERCENT\")\n",
    "reviewUGPercent = sortDict(reviewUGPercent)\n",
    "print(\"SORTING WINE UNIGRAM COUNT\")\n",
    "reviewUGCount = sortDict(reviewUGCount)\n",
    "print(\"SORTING WINE BIGRAM PERCENT\")\n",
    "reviewBGPercent = sortDict(reviewBGPercent)\n",
    "print(\"SORTING WINE BIGRAM COUNT\")\n",
    "reviewBGCount = sortDict(reviewBGCount)\n",
    "print(\"SORTING WINE TRIGRAM PERCENT\")\n",
    "reviewTGPercent = sortDict(reviewTGPercent)\n",
    "print(\"SORTING WINE TRIGRAM COUNT\")\n",
    "reviewTGCount = sortDict(reviewTGCount)\n",
    "\n",
    "print(\"WRITING WINE FILES\")\n",
    "print(\"WRITING WINE unigramCounts\")\n",
    "nGramDictToCSV(\"wine-reviews/\", \"unigramCounts\", reviewUGCount)\n",
    "print(\"WRITING WINE unigramPercent\")\n",
    "nGramDictToCSV(\"wine-reviews/\", \"unigramPercent\", reviewUGPercent)\n",
    "print(\"WRITING WINE bigramCounts\")\n",
    "nGramDictToCSV(\"wine-reviews/\", \"bigramCounts\", reviewBGCount)\n",
    "print(\"WRITING WINE bigramPercent\")\n",
    "nGramDictToCSV(\"wine-reviews/\", \"bigramPercent\", reviewBGPercent)\n",
    "print(\"WRITING WINE trigramCounts\")\n",
    "nGramDictToCSV(\"wine-reviews/\", \"trigramCounts\", reviewTGCount)\n",
    "print(\"WRITING WINE trigramPercent\")\n",
    "nGramDictToCSV(\"wine-reviews/\", \"trigramPercent\", reviewTGPercent)\n",
    "\n",
    "print(\"END WINE REVIEWS\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "###############################################################\n",
    "\n",
    "print(\"START NEWS\")\n",
    "articleTokenCount = 0    \n",
    "\n",
    "articleUnigrams = Counter()\n",
    "articleBigrams = Counter()\n",
    "articleTrigrams = Counter()\n",
    "\n",
    "print(\"CREATING NGRAM COUNTS - NEWS\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "#Prep for multithread\n",
    "interval = math.ceil(len(articleSents) / threadCount)\n",
    "\n",
    "articleThreadDict = manager.dict()\n",
    "articleUnigramsDict = manager.dict()\n",
    "articleBigramsDict = manager.dict()\n",
    "articleTrigramsDict = manager.dict()\n",
    "articleTokenCountDict = manager.dict()\n",
    "\n",
    "for x in range(threadCount):\n",
    "    articleThreadDict.update({x : articleSents[interval*x:interval*(x+1)]})\n",
    "    articleUnigramsDict.update({x : {}})\n",
    "    articleBigramsDict.update({x : {}})\n",
    "    articleTrigramsDict.update({x : {}})\n",
    "    articleTokenCountDict.update({x : 0})\n",
    "\n",
    "print(\"News sentence length: \" + str(len(reviewSents)))      \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    threads = list()\n",
    "    for index in range(threadCount):\n",
    "        x = Process(target=ngram_function, args=(index, articleThreadDict, articleTokenCountDict, articleUnigramsDict, articleBigramsDict, articleTrigramsDict, stopwords))\n",
    "        threads.append(x)\n",
    "        print(\"Starting thread \" + str(index))\n",
    "        x.start()\n",
    "\n",
    "    for a in range(threadCount):\n",
    "        thread = threads[a]\n",
    "        thread.join()\n",
    "        articleUnigrams += articleUnigramsDict[a]\n",
    "        articleBigrams += articleBigramsDict[a]\n",
    "        articleTrigrams += articleTrigramsDict[a]\n",
    "        articleTokenCount += articleTokenCountDict[a]\n",
    "        print(\"End thread \" + str(a))\n",
    "        \n",
    "print(\"All threads complete\")\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "print(\"CREATING COUNTER DICTIONARY - NEWS\")\n",
    "articleNGramCounts = {\n",
    "    \"unigrams\" : Counter(articleUnigrams),\n",
    "    \"bigrams\" : Counter(articleBigrams),\n",
    "    \"trigrams\" : Counter(articleTrigrams)\n",
    "}\n",
    "\n",
    "articleUGPercent = {}\n",
    "articleUGCount = {}\n",
    "articleBGPercent = {}\n",
    "articleBGCount = {}\n",
    "articleTGPercent = {}\n",
    "articleTGCount = {}\n",
    "\n",
    "print(\"PROCCESSING UNIGRAMS PERCENTAGE - NEWS\")\n",
    "processUnigrams(articleTokenCount, articleNGramCounts[\"unigrams\"], articleUGPercent, articleUGCount)\n",
    "print(\"PROCCESSING BIGRAM PERCENTAGE - NEWS\")\n",
    "processNGrams(articleNGramCounts[\"bigrams\"], articleUGCount, articleBGPercent, articleBGCount)\n",
    "print(\"PROCCESSING TRIGRAM PERCENTAGE - NEWS\")\n",
    "processNGrams(articleNGramCounts[\"trigrams\"], articleUGCount, articleTGPercent, articleTGCount)\n",
    "\n",
    "print(\"SORTING NEWS UNIGRAM PERCENT\")\n",
    "articleUGPercent = sortDict(articleUGPercent)\n",
    "print(\"SORTING NEWS UNIGRAM COUNT\")\n",
    "articleUGCount = sortDict(articleUGCount)\n",
    "print(\"SORTING NEWS BIGRAM PERCENT\")\n",
    "articleBGPercent = sortDict(articleBGPercent)\n",
    "print(\"SORTING NEWS BIGRAM COUNT\")\n",
    "articleBGCount = sortDict(articleBGCount)\n",
    "print(\"SORTING NEWS TRIGRAM PERCENT\")\n",
    "articleTGPercent = sortDict(articleTGPercent)\n",
    "print(\"SORTING NEWS TRIGRAM COUNT\")\n",
    "articleTGCount = sortDict(articleTGCount)\n",
    "\n",
    "print(\"WRITING NEWS FILES\")\n",
    "print(\"WRITING NEWS unigramCounts\")\n",
    "nGramDictToCSV(\"all-the-news/\", \"unigramCounts\", articleUGCount)\n",
    "print(\"WRITING NEWS unigramPercent\")\n",
    "nGramDictToCSV(\"all-the-news/\", \"unigramPercent\", articleUGPercent)\n",
    "print(\"WRITING NEWS bigramCounts\")\n",
    "nGramDictToCSV(\"all-the-news/\", \"bigramCounts\", articleBGCount)\n",
    "print(\"WRITING NEWS bigramPercent\")\n",
    "nGramDictToCSV(\"all-the-news/\", \"bigramPercent\", articleBGPercent)\n",
    "print(\"WRITING NEWS trigramCounts\")\n",
    "nGramDictToCSV(\"all-the-news/\", \"trigramCounts\", articleTGCount)\n",
    "print(\"WRITING NEWS trigramPercent\")\n",
    "nGramDictToCSV(\"all-the-news/\", \"trigramPercent\", articleTGPercent)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "print(\"END NEWS\")\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Compare the statistics of the corpora.</u>\n",
    "                        \n",
    "Use the results of those calculations that you just made the poor computer painstakingly compute. What are the differences in the most common unigrams between the two language models? Are there interesting differences between the bigram models or trigram models?\n",
    "\n",
    "Be able to sort the n-grams to output the top k with the highest count or probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a'),14062\n",
      "('on', 'the', 'finish'),10524\n",
      "('in', 'the', 'mouth'),8994\n",
      "('on', 'the', 'palate'),7633\n",
      "('the', 'wine', 'is'),7256\n",
      "('on', 'the', 'nose'),6931\n",
      "('the', 'palate', 'is'),6179\n",
      "('a', 'touch', 'of'),5641\n",
      "('a', 'hint', 'of'),4099\n",
      "('the', 'finish', 'is'),3437\n"
     ]
    }
   ],
   "source": [
    "# Load ngram percents and counts from files\n",
    "# Use head 100 files for testing\n",
    "\n",
    "#Major differences in news was mainly politcal and contained words talking about the president, white hosue, us etc\n",
    "#Wine reiviews had none of this in their top ngrams\n",
    "\n",
    "def readCountPercentFiles(fp):\n",
    "    file = open(fp, \"r\")\n",
    "    tmpDict = {}\n",
    "    with file as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "        for lines in csv_reader:\n",
    "            tmpDict.update({lines[0] : lines[1]})\n",
    "    return tmpDict\n",
    "\n",
    "def writeTopN(theDict, top):\n",
    "    for x in range(top):\n",
    "        print(list(theDict.keys())[x] + \",\" + list(theDict.values())[x])\n",
    "\n",
    "newsUGCountDict = readCountPercentFiles(\"all-the-news/unigramCounts100.csv\")\n",
    "newsBGCountDict = readCountPercentFiles(\"all-the-news/bigramCounts100.csv\")\n",
    "newsTGCountDict = readCountPercentFiles(\"all-the-news/trigramCounts100.csv\")\n",
    "newsUGPercentDict = readCountPercentFiles(\"all-the-news/unigramPercent100.csv\")\n",
    "newsBGPercentDict = readCountPercentFiles(\"all-the-news/bigramPercent100.csv\")\n",
    "newsTGPercentDict = readCountPercentFiles(\"all-the-news/trigramPercent100.csv\")\n",
    "\n",
    "wineUGCountDict = readCountPercentFiles(\"wine-reviews/unigramCounts100.csv\")\n",
    "wineBGCountDict = readCountPercentFiles(\"wine-reviews/bigramCounts100.csv\")\n",
    "wineTGCountDict = readCountPercentFiles(\"wine-reviews/trigramCounts100.csv\")\n",
    "wineUGPercentDict = readCountPercentFiles(\"wine-reviews/unigramPercent100.csv\")\n",
    "wineBGPercentDict = readCountPercentFiles(\"wine-reviews/bigramPercent100.csv\")\n",
    "wineTGPercentDict = readCountPercentFiles(\"wine-reviews/trigramPercent100.csv\")\n",
    "\n",
    "#newsUGCountDict = readCountPercentFiles(\"all-the-news/unigramCountsFull.csv\")\n",
    "#newsBGCountDict = readCountPercentFiles(\"all-the-news/bigramCountsFull.csv\")\n",
    "#newsTGCountDict = readCountPercentFiles(\"all-the-news/trigramCountsFull.csv\")\n",
    "#newsUGPercentDict = readCountPercentFiles(\"all-the-news/unigramPercentFull.csv\")\n",
    "#newsBGPercentDict = readCountPercentFiles(\"all-the-news/bigramPercentFull.csv\")\n",
    "#newsTGPercentDict = readCountPercentFiles(\"all-the-news/trigramPercentFull.csv\")\n",
    "\n",
    "#wineUGCountDict = readCountPercentFiles(\"wine-reviews/unigramCountsFull.csv\")\n",
    "#wineBGCountDict = readCountPercentFiles(\"wine-reviews/bigramCountsFull.csv\")\n",
    "#wineTGCountDict = readCountPercentFiles(\"wine-reviews/trigramCountsFull.csv\")\n",
    "#wineUGPercentDict = readCountPercentFiles(\"wine-reviews/unigramPercentFull.csv\")\n",
    "#wineBGPercentDict = readCountPercentFiles(\"wine-reviews/bigramPercentFull.csv\")\n",
    "#wineTGPercentDict = readCountPercentFiles(\"wine-reviews/trigramPercentFull.csv\")\n",
    "\n",
    "#print top N from dict\n",
    "#csv's are already sorted\n",
    "writeTopN(wineTGCountDict, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Generate random sentences from the N-grams models for both datasets.</u>\n",
    "                        \n",
    "We briefly talked about this idea in class. It's also introduced at a high-level in J&M 4.3. How can a random number in the range [0,1] probabilistically generate a word using your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a technical report (in this Jupyter Notebook, with good Markdown formatting) that documents your findings, \"lessons learned\", any areas of where you ran into difficult, and also any other interesting details. Include in your report the following details:\n",
    "\n",
    "1. Names of the datasets used.\n",
    "1. Does your model use all of the data in the .csv file or only a subset of it (i.e. first 1,000 rows)?\n",
    "1. What is the vocabulary and size of each dataset?\n",
    "1. How did you handle the merging of separate rows in a .csv file? How did you handle sentence segmentation with sentence boundary markers? Also report on any other decisions made in step #3.\n",
    "1. How long did it take your program to build these models? Do you have any statistics on memory/RAM usage?\n",
    "1. Output the top 15 unigrams, bigrams, trigrams for each model. Are there any interesting differences?\n",
    "1. Output 3 different randomly generated sentences for each unigram, bigram, trigram model. How did you know where the randomly generated sentence ended?\n",
    "\n",
    "Also submit this python notebook `.ipynb` to D2L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON CODE AND REPORT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
